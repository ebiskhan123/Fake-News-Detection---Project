{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sentence_ent():\n",
    "    \n",
    "    def __init__(self):\n",
    "        train_df = pd.read_csv('/Users/ebby/Downloads/SICK/SICK.txt', sep=\"\\t\", header=None)\n",
    "        train_df.columns = train_df.iloc[0]\n",
    "        train_df=train_df.reindex(train_df.index.drop(0))\n",
    "        EMBEDDING_FILE = '/Users/ebby/Documents/Fake news/MALSTM/GoogleNews-vectors-negative300.bin'\n",
    "        MODEL_SAVING_DIR = '/Users/ebby/Documents/Fake news/MALSTM'\n",
    "        self.word2vec_location = '/Users/ebby/Documents/Fake news/MALSTM/GoogleNews-vectors-negative300.bin'\n",
    "        \n",
    "    def preprocess(self):\n",
    "        stops = set(stopwords.words('english'))\n",
    "\n",
    "        def text_to_word_list(text):\n",
    "            ''' Pre process and convert texts to a list of words '''\n",
    "            text = str(text)\n",
    "            text = text.lower()\n",
    "\n",
    "            # Clean the text\n",
    "            text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "            text = re.sub(r\"what's\", \"what is \", text)\n",
    "            text = re.sub(r\"\\'s\", \" \", text)\n",
    "            text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "            text = re.sub(r\"can't\", \"cannot \", text)\n",
    "            text = re.sub(r\"n't\", \" not \", text)\n",
    "            text = re.sub(r\"i'm\", \"i am \", text)\n",
    "            text = re.sub(r\"\\'re\", \" are \", text)\n",
    "            text = re.sub(r\"\\'d\", \" would \", text)\n",
    "            text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "            text = re.sub(r\",\", \" \", text)\n",
    "            text = re.sub(r\"\\.\", \" \", text)\n",
    "            text = re.sub(r\"!\", \" ! \", text)\n",
    "            text = re.sub(r\"\\/\", \" \", text)\n",
    "            text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "            text = re.sub(r\"\\+\", \" + \", text)\n",
    "            text = re.sub(r\"\\-\", \" - \", text)\n",
    "            text = re.sub(r\"\\=\", \" = \", text)\n",
    "            text = re.sub(r\"'\", \" \", text)\n",
    "            text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "            text = re.sub(r\":\", \" : \", text)\n",
    "            text = re.sub(r\" e g \", \" eg \", text)\n",
    "            text = re.sub(r\" b g \", \" bg \", text)\n",
    "            text = re.sub(r\" u s \", \" american \", text)\n",
    "            text = re.sub(r\"\\0s\", \"0\", text)\n",
    "            text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "            text = re.sub(r\"e - mail\", \"email\", text)\n",
    "            text = re.sub(r\"j k\", \"jk\", text)\n",
    "            text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "            text = text.split()\n",
    "\n",
    "            return text\n",
    "\n",
    "        # Prepare embedding\n",
    "        vocabulary = dict()\n",
    "        inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
    "        word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "        questions_cols = ['sentence_A' , 'sentence_B']\n",
    "\n",
    "        # Iterate over the questions only of both training and test datasets\n",
    "        for dataset in [self.test_df]:\n",
    "            for index, row in dataset.iterrows():\n",
    "\n",
    "                # Iterate through the text of both questions of the row\n",
    "                for question in questions_cols:\n",
    "\n",
    "                    q2n = []  # q2n -> question numbers representation\n",
    "                    print(text_to_word_list(row[question]))\n",
    "                    for word in text_to_word_list(row[question]):\n",
    "\n",
    "                        # Check for unwanted words\n",
    "                        if word in stops and word not in word2vec.vocab:\n",
    "                            continue\n",
    "\n",
    "                        if word not in vocabulary:\n",
    "                            vocabulary[word] = len(inverse_vocabulary)\n",
    "                            q2n.append(len(inverse_vocabulary))\n",
    "                            inverse_vocabulary.append(word)\n",
    "                            #print(word)\n",
    "                        else:\n",
    "                            q2n.append(vocabulary[word])\n",
    "\n",
    "                    # Replace questions as word to question as number representation\n",
    "                    self.dataset.set_value(index, question, q2n)\n",
    "\n",
    "        self.embedding_dim = 300\n",
    "        self.embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "        self.embeddings[0] = 0  # So that the padding will be ignored\n",
    "        # Build the embedding matrix\n",
    "        for word, index in vocabulary.items():\n",
    "            if word in word2vec.vocab:\n",
    "                self.embeddings[index] = word2vec.word_vec(word)\n",
    "\n",
    "        del word2vec\n",
    "        \n",
    "        \n",
    "        \n",
    "    def pad_train_data_one(self):\n",
    "        max_seq_length = max(train_df.sentence_A.map(lambda x: len(x)).max(),\n",
    "                     train_df.sentence_B.map(lambda x: len(x)).max(),)\n",
    "\n",
    "        X_train = self.train_df[questions_cols]\n",
    "        Y_train = self.train_df['relatedness_score']\n",
    "        # Split to dicts\n",
    "        X_train = {'left': X_train.sentence_A, 'right': X_train.sentence_B}\n",
    "\n",
    "        # Convert labels to their numpy representations\n",
    "        Y_train = Y_train.values\n",
    "\n",
    "        # Zero padding\n",
    "        for dataset, side in itertools.product([X_train], ['left', 'right']):\n",
    "            self.dataset[side] = pad_sequences(self.dataset[side], maxlen=max_seq_length)\n",
    "        \n",
    "        \n",
    "    def load_dep(self) : \n",
    "        file = open(self.vocab_location,\"rb\")\n",
    "        self.vocabulary = pickle.load(file)\n",
    "        file.close()\n",
    "        file = open(self.embeddings_location, \"rb\")\n",
    "        self.embeddings = pickle.load(file)\n",
    "        file.close()\n",
    "        print(\"Dependencies loaded!\")\n",
    "        \n",
    "    def to_index_and_pad(self) : \n",
    "        \n",
    "        self.index_rep = []\n",
    "        for word in self.word_list : \n",
    "            if word in self.vocabulary : \n",
    "                self.index_rep.append(vocabulary[word])\n",
    "        self.index_rep = pad_sequences([self.index_rep], maxlen=26)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ENTAILMENT():\n",
    "    \n",
    "    def __init__(self) : \n",
    "        S = Sentence_ent()\n",
    "        S.load_dep()\n",
    "        self.sentence = S\n",
    "        \n",
    "        \n",
    "    def load_model_malstm(self):\n",
    "        n_hidden = 50\n",
    "        gradient_clipping_norm = 1\n",
    "        batch_size = 64\n",
    "        n_epoch = 25\n",
    "\n",
    "        def exponent_neg_manhattan_distance(left, right):\n",
    "\n",
    "            ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "            return (K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True)))*4.0 +1\n",
    "\n",
    "\n",
    "        # The visible layer\n",
    "        left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "        right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "        embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
    "\n",
    "        # Embedded version of the inputs\n",
    "        encoded_left = embedding_layer(left_input)\n",
    "        encoded_right = embedding_layer(right_input)\n",
    "\n",
    "        # Since this is a siamese network, both sides share the same LSTM\n",
    "        shared_lstm = LSTM(n_hidden)\n",
    "\n",
    "        left_output = shared_lstm(encoded_left)\n",
    "        right_output = shared_lstm(encoded_right)\n",
    "\n",
    "        # Calculates the distance as defined by the MaLSTM model\n",
    "        malstm_distance = Merge(mode=lambda x: ((K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True)))*4.0 +1), output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "        # Pack it all up into a model\n",
    "        self.malstm = Model([left_input, right_input], [malstm_distance])\n",
    "\n",
    "\n",
    "        # Adadelta optimizer, with gradient clipping by norm\n",
    "        optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
    "\n",
    "        self.malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error'])\n",
    "\n",
    "        # Start training\n",
    "        training_start_time = time()\n",
    "\n",
    "        self.malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch)\n",
    "\n",
    "        print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))\n",
    "\n",
    "    def get_parameters(self,sentence_A_processed,sentence_B_processed):\n",
    "        features_function = K.function([left_input,right_input], [left_output,right_output])\n",
    "        features = features_function([sentence_A_processed,sentence_B_processed])\n",
    "        np_feature_difference = np.array(features)\n",
    "        left_matrix = np_feature_difference[0]\n",
    "        right_matrix = np_feature_difference[1]\n",
    "        diff_features= np.abs(np.subtract(left_matrix, right_matrix))\n",
    "        prod = np.multiply(left_matrix,right_matrix)\n",
    "        self.extracted_features = np.concatenate([diff_features,prod], axis = 1)\n",
    "    \n",
    "    def load_model_entailment(self):\n",
    "        self.dense_model = keras.models.load_model('CONTRADICTION_PREDICTION.h5')\n",
    "    \n",
    "    def entailment_prediction(self):\n",
    "        pred_val =self.dense_model.predict(self.extracted_features)\n",
    "        print(pred_val)\n",
    "        return pred_val        \n",
    "        \n",
    "    def predict_value(self, A, B):\n",
    "#         \"CONTRADICTION\" : 0 , \"ENTAILMENT\" : 1 ,\"NEUTRAL\" : 2\n",
    "        S = self.sentence\n",
    "        \n",
    "        S.sentence = A\n",
    "        A = S.preprocess()\n",
    "        S.sentence = B\n",
    "        B = S.preprocess()        \n",
    "        \n",
    "        sentence_A_processed = A\n",
    "        sentence_B_processed = B \n",
    "        self.load_model_malstm()\n",
    "        self.get_parameters(sentence_A_processed,sentence_B_processed)\n",
    "        self.load_model_entailment()\n",
    "        entailment = self.entailment_prediction()\n",
    "        return entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Input, Embedding, LSTM, Merge,merge,Dense,Dropout\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta,Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def PREDICT_CONTRADICTION(test):\n",
    "    train_df = pd.read_csv('/Users/ebby/Downloads/SICK/SICK.txt', sep=\"\\t\", header=None)\n",
    "    train_df.columns = train_df.iloc[0]\n",
    "    train_df=train_df.reindex(train_df.index.drop(0))\n",
    "    EMBEDDING_FILE = '/Users/ebby/Documents/Fake news/MALSTM/GoogleNews-vectors-negative300.bin'\n",
    "    MODEL_SAVING_DIR = '/Users/ebby/Documents/Fake news/MALSTM'\n",
    "    word2vec_location = '/Users/ebby/Documents/Fake news/MALSTM/GoogleNews-vectors-negative300.bin'\n",
    "    stops = set(stopwords.words('english'))\n",
    "    test_df = test#pd.read_csv('test file location', sep=\"\\t\", header=None)\n",
    "    vocabulary = dict()\n",
    "    inverse_vocabulary = ['<unk>']  \n",
    "    word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "    questions_cols = ['sentence_A' , 'sentence_B']\n",
    "\n",
    "    for dataset in [train_df,test_df]:\n",
    "        for index, row in dataset.iterrows():\n",
    "            for question in questions_cols:\n",
    "\n",
    "                q2n = []  \n",
    "                for word in text_to_word_list(row[question]):\n",
    "\n",
    "                    # Check for unwanted words\n",
    "                    if word in stops and word not in word2vec.vocab:\n",
    "                        continue\n",
    "\n",
    "                    if word not in vocabulary:\n",
    "                        vocabulary[word] = len(inverse_vocabulary)\n",
    "                        q2n.append(len(inverse_vocabulary))\n",
    "                        inverse_vocabulary.append(word)\n",
    "                        #print(word)\n",
    "                    else:\n",
    "                        q2n.append(vocabulary[word])\n",
    "                dataset.set_value(index, question, q2n)\n",
    "\n",
    "    embedding_dim = 300\n",
    "    embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "    embeddings[0] = 0 \n",
    "    for word, index in vocabulary.items():\n",
    "        if word in word2vec.vocab:\n",
    "            embeddings[index] = word2vec.word_vec(word)\n",
    "\n",
    "    del word2vec\n",
    "    \n",
    "    max_seq_length = max(train_df.sentence_A.map(lambda x: len(x)).max(),\n",
    "                         train_df.sentence_B.map(lambda x: len(x)).max(),\n",
    "                         test_df.sentence_A.map(lambda x: len(x)).max(),\n",
    "                         test_df.sentence_B.map(lambda x: len(x)).max())\n",
    "\n",
    "    X_train = train_df[questions_cols]\n",
    "    Y_train = train_df['relatedness_score']\n",
    "    X_test = test_df[questions_cols]\n",
    "    X_train = {'left': X_train.sentence_A, 'right': X_train.sentence_B}\n",
    "    X_test = {'left': test_df.sentence_A, 'right': test_df.sentence_B}\n",
    "    Y_train = Y_train.values\n",
    "    for dataset, side in itertools.product([X_train], ['left', 'right']):\n",
    "        dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
    "        \n",
    "    X_train1 = train_df[questions_cols]\n",
    "    Y_train1 = train_df['entailment_label']\n",
    "    X_train1 = {'left': X_train1.sentence_A, 'right': X_train1.sentence_B}\n",
    "    Y_train1 = Y_train1.values\n",
    "    X_test1 = test_df[questions_cols]\n",
    "    X_test1 = {'left': test_df.sentence_A, 'right': test_df.sentence_B}\n",
    "\n",
    "\n",
    "    for dataset, side in itertools.product([X_train1,X_test1], ['left', 'right']):\n",
    "        dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
    "        \n",
    "    n_hidden = 50\n",
    "    gradient_clipping_norm = 1\n",
    "    batch_size = 64\n",
    "    n_epoch = 25\n",
    "\n",
    "    def exponent_neg_manhattan_distance(left, right):\n",
    "        return (K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True)))*4.0 +1\n",
    "\n",
    "    left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "    right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "    embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
    "    encoded_left = embedding_layer(left_input)\n",
    "    encoded_right = embedding_layer(right_input)\n",
    "    shared_lstm = LSTM(n_hidden)\n",
    "    left_output = shared_lstm(encoded_left)\n",
    "    right_output = shared_lstm(encoded_right)\n",
    "    malstm_distance = Merge(mode=lambda x: ((K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True)))*4.0 +1), output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "    malstm = Model([left_input, right_input], [malstm_distance])\n",
    "    optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
    "    malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error'])\n",
    "    training_start_time = time()\n",
    "    malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch)\n",
    "    print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))\n",
    "\n",
    "    features_function = K.function([left_input,right_input], [left_output,right_output])\n",
    "    features = features_function([X_train1['left'],X_train1['right']])\n",
    "    np_feature_difference = np.array(features)\n",
    "    left_matrix = np_feature_difference[0]\n",
    "    right_matrix = np_feature_difference[1]\n",
    "    diff_features= np.abs(np.subtract(left_matrix, right_matrix))\n",
    "    prod = np.multiply(left_matrix,right_matrix)\n",
    "    extracted_features = np.concatenate([diff_features,prod], axis = 1)\n",
    "    \n",
    "    features = features_function([X_test1['left'],X_test1['right']])\n",
    "    np_feature_difference = np.array(features)\n",
    "    left_matrix = np_feature_difference[0]\n",
    "    right_matrix = np_feature_difference[1]\n",
    "    diff_features= np.abs(np.subtract(left_matrix, right_matrix))\n",
    "    prod = np.multiply(left_matrix,right_matrix)\n",
    "    extracted_features_test = np.concatenate([diff_features,prod], axis = 1)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_shape=(100,),kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(Adam(lr = 0.001),'categorical_crossentropy', metrics=['accuracy'])\n",
    "    y_TRAIN_cat = pd.get_dummies(Y_train1)\n",
    "    model.fit(extracted_features, y_TRAIN_cat.values,batch_size=64,epochs=5,verbose=1)\n",
    "    print(\"Entailmet model finished training\")\n",
    "    \n",
    "    pred_val =model.predict_classes(extracted_features_test)\n",
    "    \n",
    "    return pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:126: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:131: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "9840/9840 [==============================] - 18s 2ms/step - loss: 1.1110 - mean_squared_error: 1.1110\n",
      "Epoch 2/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.4996 - mean_squared_error: 0.4996\n",
      "Epoch 3/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.3902 - mean_squared_error: 0.3902\n",
      "Epoch 4/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.3331 - mean_squared_error: 0.3331\n",
      "Epoch 5/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.2992 - mean_squared_error: 0.2992\n",
      "Epoch 6/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.2761 - mean_squared_error: 0.2761\n",
      "Epoch 7/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.2595 - mean_squared_error: 0.2595\n",
      "Epoch 8/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.2476 - mean_squared_error: 0.2476\n",
      "Epoch 9/25\n",
      "9840/9840 [==============================] - 432s 44ms/step - loss: 0.2377 - mean_squared_error: 0.2377\n",
      "Epoch 10/25\n",
      "9840/9840 [==============================] - 22s 2ms/step - loss: 0.2278 - mean_squared_error: 0.2278\n",
      "Epoch 11/25\n",
      "9840/9840 [==============================] - 18s 2ms/step - loss: 0.2212 - mean_squared_error: 0.2212\n",
      "Epoch 12/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.2142 - mean_squared_error: 0.2142\n",
      "Epoch 13/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.2071 - mean_squared_error: 0.2071\n",
      "Epoch 14/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.2027 - mean_squared_error: 0.2027\n",
      "Epoch 15/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.1978 - mean_squared_error: 0.1978\n",
      "Epoch 16/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.1926 - mean_squared_error: 0.1926\n",
      "Epoch 17/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.1879 - mean_squared_error: 0.1879\n",
      "Epoch 18/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.1841 - mean_squared_error: 0.1841\n",
      "Epoch 19/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.1804 - mean_squared_error: 0.1804\n",
      "Epoch 20/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.1767 - mean_squared_error: 0.1767\n",
      "Epoch 21/25\n",
      "9840/9840 [==============================] - 19s 2ms/step - loss: 0.1738 - mean_squared_error: 0.1738\n",
      "Epoch 22/25\n",
      "9840/9840 [==============================] - 18s 2ms/step - loss: 0.1702 - mean_squared_error: 0.1702\n",
      "Epoch 23/25\n",
      "9840/9840 [==============================] - 18s 2ms/step - loss: 0.1677 - mean_squared_error: 0.1677\n",
      "Epoch 24/25\n",
      "9840/9840 [==============================] - 18s 2ms/step - loss: 0.1644 - mean_squared_error: 0.1644\n",
      "Epoch 25/25\n",
      "9840/9840 [==============================] - 18s 2ms/step - loss: 0.1629 - mean_squared_error: 0.1629\n",
      "Training time finished.\n",
      "25 epochs in 0:14:15.974911\n",
      "Epoch 1/5\n",
      "9840/9840 [==============================] - 1s 100us/step - loss: 0.6653 - acc: 0.7149\n",
      "Epoch 2/5\n",
      "9840/9840 [==============================] - 1s 81us/step - loss: 0.3936 - acc: 0.8569\n",
      "Epoch 3/5\n",
      "9840/9840 [==============================] - 1s 78us/step - loss: 0.3742 - acc: 0.8625\n",
      "Epoch 4/5\n",
      "9840/9840 [==============================] - 1s 89us/step - loss: 0.3664 - acc: 0.8649\n",
      "Epoch 5/5\n",
      "9840/9840 [==============================] - 1s 81us/step - loss: 0.3575 - acc: 0.8713\n",
      "Entailmet model finished training\n"
     ]
    }
   ],
   "source": [
    "pred =  PREDICT_CONTRADICTION(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame([[\"i am running away from home\",\"i am flying to USA\"],['The boy died in the truck' ,'The boy did not die in the truck'],['Today is Monday', 'This day is Monday'],[\"Assissins Creed will be releasing later this may\",\"Assasins creed will not be releasing later this year\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.columns =[\"sentence_A\",\"sentence_B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dictionary ={'NEUTRAL' : 2 , 'ENTAILMENT' : 1 , 'CONTRADICTION' :0}\n",
    "test_df[\"3\"] = test_df[\"3\"].map(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = test_df[\"3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(heading1,columns=[\"sentence_A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df[\"sentence_B\"]=fake[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello</td>\n",
       "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>world</td>\n",
       "      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_A                                         sentence_B\n",
       "0      hello  Muslims BUSTED: They Stole Millions In Gov’t B...\n",
       "1      world  Re: Why Did Attorney General Loretta Lynch Ple..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset =pd.DataFrame()\n",
    "#for loop here\n",
    "#code here\n",
    "crawled_data = pd.DataFrame(crawled_list,columns=[\"heading_crawled\",\"article_crawled\"])\n",
    "crawled_data[\"fake_title\"] = title_looped\n",
    "#end_of one loop\n",
    "news_dataset = news_dataset.append(crawled_data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "'''\n",
    "300D Model - Train / Test (epochs)\n",
    "=-=-=\n",
    "Batch size = 512\n",
    "Fixed GloVe\n",
    "- 300D SumRNN + Translate + 3 MLP (1.2 million parameters) - 0.8315 / 0.8235 / 0.8249 (22 epochs)\n",
    "- 300D GRU + Translate + 3 MLP (1.7 million parameters) - 0.8431 / 0.8303 / 0.8233 (17 epochs)\n",
    "- 300D LSTM + Translate + 3 MLP (1.9 million parameters) - 0.8551 / 0.8286 / 0.8229 (23 epochs)\n",
    "Following Liu et al. 2016, I don't update the GloVe embeddings during training.\n",
    "Unlike Liu et al. 2016, I don't initialize out of vocabulary embeddings randomly and instead leave them zeroed.\n",
    "The jokingly named SumRNN (summation of word embeddings) is 10-11x faster than the GRU or LSTM.\n",
    "Original numbers for sum / LSTM from Bowman et al. '15 and Bowman et al. '16\n",
    "=-=-=\n",
    "100D Sum + GloVe - 0.793 / 0.753\n",
    "100D LSTM + GloVe - 0.848 / 0.776\n",
    "300D LSTM + GloVe - 0.839 / 0.806\n",
    "'''\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import merge, recurrent, Dense, Input, Dropout, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def extract_tokens_from_binary_parse(parse):\n",
    "\t\treturn parse.replace('(', ' ').replace(')', ' ').replace('-LRB-', '(').replace('-RRB-', ')').split()\n",
    "\n",
    "def yield_examples(fn, skip_no_majority=True, limit=None):\n",
    "\tfor i, line in enumerate(open(fn)):\n",
    "\t\tif limit and i > limit:\n",
    "\t\t\tbreak\n",
    "\t\tdata = json.loads(line)\n",
    "\t\tlabel = data['gold_label']\n",
    "\t\ts1 = ' '.join(extract_tokens_from_binary_parse(data['sentence1_binary_parse']))\n",
    "\t\ts2 = ' '.join(extract_tokens_from_binary_parse(data['sentence2_binary_parse']))\n",
    "\t\tif skip_no_majority and label == '-':\n",
    "\t\t\tcontinue\n",
    "\t\tyield (label, s1, s2)\n",
    "\n",
    "def get_data(fn, limit=None):\n",
    "\traw_data = list(yield_examples(fn=fn, limit=limit))\n",
    "\tleft = [s1 for _, s1, s2 in raw_data]\n",
    "\tright = [s2 for _, s1, s2 in raw_data]\n",
    "\tprint(max(len(x.split()) for x in left))\n",
    "\tprint(max(len(x.split()) for x in right))\n",
    "\n",
    "\tLABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
    "\tY = np.array([LABELS[l] for l, s1, s2 in raw_data])\n",
    "\tY = np_utils.to_categorical(Y, len(LABELS))\n",
    "\n",
    "\treturn left, right, Y\n",
    "\n",
    "training = get_data('snli_1.0_train.jsonl')\n",
    "validation = get_data('snli_1.0_dev.jsonl')\n",
    "test = get_data('snli_1.0_test.jsonl')\n",
    "\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(training[0] + training[1])\n",
    "\n",
    "# Lowest index from the tokenizer is 1 - we need to include 0 in our vocab count\n",
    "VOCAB = len(tokenizer.word_counts) + 1\n",
    "LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
    "#RNN = recurrent.LSTM\n",
    "#RNN = lambda *args, **kwargs: Bidirectional(recurrent.LSTM(*args, **kwargs))\n",
    "#RNN = recurrent.GRU\n",
    "#RNN = lambda *args, **kwargs: Bidirectional(recurrent.GRU(*args, **kwargs))\n",
    "# Summation of word embeddings\n",
    "RNN = None\n",
    "LAYERS = 1\n",
    "USE_GLOVE = True\n",
    "TRAIN_EMBED = False\n",
    "EMBED_HIDDEN_SIZE = 300\n",
    "SENT_HIDDEN_SIZE = 300\n",
    "BATCH_SIZE = 512\n",
    "PATIENCE = 4 # 8\n",
    "MAX_EPOCHS = 42\n",
    "MAX_LEN = 42\n",
    "DP = 0.2\n",
    "L2 = 4e-6\n",
    "ACTIVATION = 'relu'\n",
    "OPTIMIZER = 'rmsprop'\n",
    "print('RNN / Embed / Sent = {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE))\n",
    "print('GloVe / Trainable Word Embeddings = {}, {}'.format(USE_GLOVE, TRAIN_EMBED))\n",
    "\n",
    "to_seq = lambda X: pad_sequences(tokenizer.texts_to_sequences(X), maxlen=MAX_LEN)\n",
    "prepare_data = lambda data: (to_seq(data[0]), to_seq(data[1]), data[2])\n",
    "\n",
    "training = prepare_data(training)\n",
    "validation = prepare_data(validation)\n",
    "test = prepare_data(test)\n",
    "\n",
    "print('Build model...')\n",
    "print('Vocab size =', VOCAB)\n",
    "\n",
    "GLOVE_STORE = 'precomputed_glove.weights'\n",
    "if USE_GLOVE:\n",
    "\tif not os.path.exists(GLOVE_STORE + '.npy'):\n",
    "\t\tprint('Computing GloVe')\n",
    "\t\n",
    "\t\tembeddings_index = {}\n",
    "\t\tf = open('glove.840B.300d.txt')\n",
    "\t\tfor line in f:\n",
    "\t\t\tvalues = line.split(' ')\n",
    "\t\t\tword = values[0]\n",
    "\t\t\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\t\t\tembeddings_index[word] = coefs\n",
    "\t\tf.close()\n",
    "\t\t\n",
    "\t\t# prepare embedding matrix\n",
    "\t\tembedding_matrix = np.zeros((VOCAB, EMBED_HIDDEN_SIZE))\n",
    "\t\tfor word, i in tokenizer.word_index.items():\n",
    "\t\t\tembedding_vector = embeddings_index.get(word)\n",
    "\t\t\tif embedding_vector is not None:\n",
    "\t\t\t\t# words not found in embedding index will be all-zeros.\n",
    "\t\t\t\tembedding_matrix[i] = embedding_vector\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint('Missing from GloVe: {}'.format(word))\n",
    "\t\n",
    "\t\tnp.save(GLOVE_STORE, embedding_matrix)\n",
    "\n",
    "\tprint('Loading GloVe')\n",
    "\tembedding_matrix = np.load(GLOVE_STORE + '.npy')\n",
    "\n",
    "\tprint('Total number of null word embeddings:')\n",
    "\tprint(np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "\tembed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, weights=[embedding_matrix], input_length=MAX_LEN, trainable=TRAIN_EMBED)\n",
    "else:\n",
    "\tembed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, input_length=MAX_LEN)\n",
    "\n",
    "rnn_kwargs = dict(output_dim=SENT_HIDDEN_SIZE, dropout_W=DP, dropout_U=DP)\n",
    "SumEmbeddings = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=(SENT_HIDDEN_SIZE, ))\n",
    "\n",
    "translate = TimeDistributed(Dense(SENT_HIDDEN_SIZE, activation=ACTIVATION))\n",
    "\n",
    "premise = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "hypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "\n",
    "prem = embed(premise)\n",
    "hypo = embed(hypothesis)\n",
    "\n",
    "prem = translate(prem)\n",
    "hypo = translate(hypo)\n",
    "\n",
    "if RNN and LAYERS > 1:\n",
    "\tfor l in range(LAYERS - 1):\n",
    "\t\trnn = RNN(return_sequences=True, **rnn_kwargs)\n",
    "\t\tprem = BatchNormalization()(rnn(prem))\n",
    "\t\thypo = BatchNormalization()(rnn(hypo))\n",
    "rnn = SumEmbeddings if not RNN else RNN(return_sequences=False, **rnn_kwargs)\n",
    "prem = rnn(prem)\n",
    "hypo = rnn(hypo)\n",
    "prem = BatchNormalization()(prem)\n",
    "hypo = BatchNormalization()(hypo)\n",
    "\n",
    "joint = merge([prem, hypo], mode='concat')\n",
    "joint = Dropout(DP)(joint)\n",
    "for i in range(3):\n",
    "\tjoint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, W_regularizer=l2(L2) if L2 else None)(joint)\n",
    "\tjoint = Dropout(DP)(joint)\n",
    "\tjoint = BatchNormalization()(joint)\n",
    "\n",
    "pred = Dense(len(LABELS), activation='softmax')(joint)\n",
    "\n",
    "model = Model(input=[premise, hypothesis], output=pred)\n",
    "model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Training')\n",
    "_, tmpfn = tempfile.mkstemp()\n",
    "# Save the best model during validation and bail out of training early if we're not improving\n",
    "callbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(tmpfn, save_best_only=True, save_weights_only=True)]\n",
    "model.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, nb_epoch=MAX_EPOCHS, validation_data=([validation[0], validation[1]], validation[2]), callbacks=callbacks)\n",
    "\n",
    "# Restore the best found model during validation\n",
    "model.load_weights(tmpfn)\n",
    "\n",
    "loss, acc = model.evaluate([test[0], test[1]], test[2], batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Input, Embedding, LSTM, Merge,merge,Dense,Dropout\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta,Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/ebby/Downloads/SICK/SICK.txt', sep=\"\\t\", header=None)\n",
    "df.columns = df.iloc[0]\n",
    "df=df.reindex(df.index.drop(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df= df.loc[df['SemEval_set'].isin(['TRAIN','TRIAL'])]\n",
    "test_df= df[df[\"SemEval_set\"]== \"TEST\"]\n",
    "EMBEDDING_FILE = '/Users/ebby/Documents/Fake news/MALSTM/GoogleNews-vectors-negative300.bin'\n",
    "MODEL_SAVING_DIR = '/Users/ebby/Documents/Fake news/MALSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Prepare embedding\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "questions_cols = ['sentence_A' , 'sentence_B']\n",
    "\n",
    "# Iterate over the questions only of both training and test datasets\n",
    "for dataset in [train_df, test_df]:\n",
    "    for index, row in dataset.iterrows():\n",
    "\n",
    "        # Iterate through the text of both questions of the row\n",
    "        for question in questions_cols:\n",
    "\n",
    "            q2n = []  # q2n -> question numbers representation\n",
    "            for word in text_to_word_list(row[question]):\n",
    "\n",
    "                # Check for unwanted words\n",
    "                if word in stops and word not in word2vec.vocab:\n",
    "                    continue\n",
    "\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(inverse_vocabulary)\n",
    "                    q2n.append(len(inverse_vocabulary))\n",
    "                    inverse_vocabulary.append(word)\n",
    "                else:\n",
    "                    q2n.append(vocabulary[word])\n",
    "\n",
    "            # Replace questions as word to question as number representation\n",
    "            dataset.set_value(index, question, q2n)\n",
    "            \n",
    "embedding_dim = 300\n",
    "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "# Build the embedding matrix\n",
    "for word, index in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embeddings[index] = word2vec.word_vec(word)\n",
    "\n",
    "del word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_seq_length = max(train_df.sentence_A.map(lambda x: len(x)).max(),\n",
    "                     train_df.sentence_B.map(lambda x: len(x)).max(),\n",
    "                     test_df.sentence_A.map(lambda x: len(x)).max(),\n",
    "                     test_df.sentence_B.map(lambda x: len(x)).max())\n",
    "\n",
    "X_train = train_df[questions_cols]\n",
    "Y_train = train_df['relatedness_score']\n",
    "X_test = test_df[questions_cols]\n",
    "Y_test = test_df['relatedness_score']\n",
    "\n",
    "# Split to dicts\n",
    "X_train = {'left': X_train.sentence_A, 'right': X_train.sentence_B}\n",
    "X_test = {'left': test_df.sentence_A, 'right': test_df.sentence_B}\n",
    "\n",
    "# Convert labels to their numpy representations\n",
    "Y_train = Y_train.values\n",
    "Y_test = Y_test.values\n",
    "\n",
    "# Zero padding\n",
    "for dataset, side in itertools.product([X_train, X_test], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
    "\n",
    "# Make sure everything is ok\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train1 = train_df[questions_cols]\n",
    "Y_train1 = train_df['entailment_label']\n",
    "X_test1 = test_df[questions_cols]\n",
    "Y_test1 = test_df['entailment_label']\n",
    "\n",
    "\n",
    "# Split to dicts\n",
    "X_train1 = {'left': X_train1.sentence_A, 'right': X_train1.sentence_B}\n",
    "X_test1 = {'left': test_df.sentence_A, 'right': test_df.sentence_B}\n",
    "\n",
    "# Convert labels to their numpy representations\n",
    "Y_train1 = Y_train1.values\n",
    "Y_test1 = Y_test1.values\n",
    "\n",
    "# Zero padding\n",
    "for dataset, side in itertools.product([X_train1, X_test1], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
    "\n",
    "# Make sure everything is ok\n",
    "assert X_train1['left'].shape == X_train1['right'].shape\n",
    "assert len(X_train1['left']) == len(Y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 1.5156 - mean_squared_error: 1.5156\n",
      "Epoch 2/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.6498 - mean_squared_error: 0.6498\n",
      "Epoch 3/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.4869 - mean_squared_error: 0.4869\n",
      "Epoch 4/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.4050 - mean_squared_error: 0.4050\n",
      "Epoch 5/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.3526 - mean_squared_error: 0.3526\n",
      "Epoch 6/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.3169 - mean_squared_error: 0.3169\n",
      "Epoch 7/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.2891 - mean_squared_error: 0.2891\n",
      "Epoch 8/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.2674 - mean_squared_error: 0.2674\n",
      "Epoch 9/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.2505 - mean_squared_error: 0.2505\n",
      "Epoch 10/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.2383 - mean_squared_error: 0.2383\n",
      "Epoch 11/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.2270 - mean_squared_error: 0.2270\n",
      "Epoch 12/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.2175 - mean_squared_error: 0.2175\n",
      "Epoch 13/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.2082 - mean_squared_error: 0.2082\n",
      "Epoch 14/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.2022 - mean_squared_error: 0.2022\n",
      "Epoch 15/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.1951 - mean_squared_error: 0.1951\n",
      "Epoch 16/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.1906 - mean_squared_error: 0.1906\n",
      "Epoch 17/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.1835 - mean_squared_error: 0.1835\n",
      "Epoch 18/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.1787 - mean_squared_error: 0.1787\n",
      "Epoch 19/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.1745 - mean_squared_error: 0.1745\n",
      "Epoch 20/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.1713 - mean_squared_error: 0.1713\n",
      "Epoch 21/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.1677 - mean_squared_error: 0.1677\n",
      "Epoch 22/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.1633 - mean_squared_error: 0.1633\n",
      "Epoch 23/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.1602 - mean_squared_error: 0.1602\n",
      "Epoch 24/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.1561 - mean_squared_error: 0.1561\n",
      "Epoch 25/25\n",
      "4934/4934 [==============================] - 9s 2ms/step - loss: 0.1546 - mean_squared_error: 0.1546\n",
      "Training time finished.\n",
      "25 epochs in 0:03:39.667448\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 50\n",
    "gradient_clipping_norm = 1\n",
    "batch_size = 64\n",
    "n_epoch = 25\n",
    "\n",
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    \n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "    return (K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True)))*4.0 +1\n",
    "\n",
    "def exponent_neg_manhattan_distance1(inputs):\n",
    "    \n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "    return (K.exp(-K.sum(inputs, axis=1, keepdims=True)))*4.0 +1\n",
    "\n",
    "\n",
    "def Man_diff(left, right):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "    return K.abs((left-right))\n",
    "def Man_prod(left, right):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "    return K.prod((left*right), axis=1, keepdims=True)\n",
    "\n",
    "# The visible layer\n",
    "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
    "\n",
    "# Embedded version of the inputs\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same LSTM\n",
    "shared_lstm = LSTM(n_hidden)\n",
    "\n",
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)\n",
    "\n",
    "# Calculates the distance as defined by the MaLSTM model\n",
    "malstm_distance = Merge(mode=lambda x: exponent_neg_manhattan_distance(x[0], x[1]), output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "# Pack it all up into a model\n",
    "malstm = Model([left_input, right_input], [malstm_distance])\n",
    "\n",
    "\n",
    "# Adadelta optimizer, with gradient clipping by norm\n",
    "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
    "\n",
    "malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error'])\n",
    "\n",
    "# Start training\n",
    "training_start_time = time()\n",
    "\n",
    "malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch)\n",
    "\n",
    "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_truth =malstm.predict([X_test['left'],X_test['right']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35345161647497408"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(Y_test,pred_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intermediate_layer_model = Model(inputs=[malstm.input[0]], outputs=[malstm.layers[4].get_input_at(0)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 26)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 26, 300)           691200    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                70200     \n",
      "=================================================================\n",
      "Total params: 761,400\n",
      "Trainable params: 70,200\n",
      "Non-trainable params: 691,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "intermediate_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"input_1:0\", shape=(?, 26), dtype=int32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    929\u001b[0m             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\n\u001b[0;32m--> 930\u001b[0;31m                                                     allow_operation=False)\n\u001b[0m\u001b[1;32m    931\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2413\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2414\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2492\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2493\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2494\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"input_1:0\", shape=(?, 26), dtype=int32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-5e6c94602ff9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mleft_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediate_layer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mright_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediate_layer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdiff_features\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mextracted_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdiff_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1789\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2458\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2459\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2460\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2461\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    931\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n\u001b[0;32m--> 933\u001b[0;31m                             + e.args[0])\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"input_1:0\", shape=(?, 26), dtype=int32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "left_matrix = intermediate_layer_model.predict([X_train1['left']])\n",
    "right_matrix = intermediate_layer_model.predict(X_train['right'])\n",
    "diff_features= np.abs(np.subtract(left_matrix, right_matrix))\n",
    "prod = np.multiply(left_matrix,right_matrix)\n",
    "extracted_features = np.concatenate([diff_features,prod], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_matrix = intermediate_layer_model.predict([X_test1['left']])\n",
    "right_matrix = intermediate_layer_model.predict(X_test1['right'])\n",
    "diff_features= np.abs(np.subtract(left_matrix, right_matrix))\n",
    "prod = np.multiply(left_matrix,right_matrix)\n",
    "extracted_features_test = np.concatenate([diff_features,prod], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor Tensor(\"lstm_1/TensorArrayReadV3:0\", shape=(?, 50), dtype=float32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-07034c4fe844>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleft_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp_feature_difference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mleft_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_feature_difference\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mright_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_feature_difference\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[1;32m   2482\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Invalid argument \"%s\" passed to K.function with TensorFlow backend'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, updates, name, **session_kwargs)\u001b[0m\n\u001b[1;32m   2425\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2426\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2427\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2428\u001b[0m             \u001b[0mupdates_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcontrol_dependencies\u001b[0;34m(control_inputs)\u001b[0m\n\u001b[1;32m   3593\u001b[0m    \u001b[0moperations\u001b[0m \u001b[0mconstructed\u001b[0m \u001b[0mwithin\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3594\u001b[0m   \"\"\"\n\u001b[0;32m-> 3595\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrol_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcontrol_dependencies\u001b[0;34m(self, control_inputs)\u001b[0m\n\u001b[1;32m   3322\u001b[0m     \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3323\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontrol_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3324\u001b[0;31m       \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3325\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3326\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2413\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2414\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2416\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2491\u001b[0m       \u001b[0;31m# Actually obj is just the object it's referring to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2492\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2493\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2494\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2495\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOperation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"lstm_1/TensorArrayReadV3:0\", shape=(?, 50), dtype=float32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "features_function = K.function([left_input,right_input], [left_output,right_output])\n",
    "features = features_function([X_train1['left'],X_train1['right']])\n",
    "np_feature_difference = np.array(features)\n",
    "left_matrix = np_feature_difference[0]\n",
    "right_matrix = np_feature_difference[1]\n",
    "diff_features= np.abs(np.subtract(left_matrix, right_matrix))\n",
    "prod = np.multiply(left_matrix,right_matrix)\n",
    "extracted_features = np.concatenate([diff_features,prod], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = features_function([X_test1['left'],X_test1['right']])\n",
    "np_feature_difference = np.array(features)\n",
    "left_matrix = np_feature_difference[0]\n",
    "right_matrix = np_feature_difference[1]\n",
    "diff_features= np.abs(np.subtract(left_matrix, right_matrix))\n",
    "prod = np.multiply(left_matrix,right_matrix)\n",
    "extracted_features_val = np.concatenate([diff_features,prod], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_shape=(100,),\n",
    "                kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(50, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(Adam(lr = 0.001),\n",
    "              'categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_TRAIN_cat = pd.get_dummies(Y_train1)\n",
    "y_TEST_cat = pd.get_dummies(Y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4934/4934 [==============================] - 2s 313us/step - loss: 0.6588 - acc: 0.7284\n",
      "Epoch 2/5\n",
      "4934/4934 [==============================] - 1s 285us/step - loss: 0.4156 - acc: 0.8525\n",
      "Epoch 3/5\n",
      "4934/4934 [==============================] - 1s 286us/step - loss: 0.3912 - acc: 0.8561\n",
      "Epoch 4/5\n",
      "4934/4934 [==============================] - 1s 281us/step - loss: 0.3821 - acc: 0.8614\n",
      "Epoch 5/5\n",
      "4934/4934 [==============================] - 1s 281us/step - loss: 0.3755 - acc: 0.8650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x126fe1cf8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(extracted_features, y_TRAIN_cat.values, batch_size=16, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79800244598450876"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_val =model.predict_classes(extracted_features_test)\n",
    "dictionary = {\"CONTRADICTION\" : 0 , \"ENTAILMENT\" : 1 ,\"NEUTRAL\" : 2}\n",
    "Y_test1_classes = test_df[\"entailment_label\"].map(dictionary)\n",
    "accuracy_score(pred_val,Y_test1_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
