{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Input, Embedding, LSTM, Merge, Dense,Concatenate,merge, multiply\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import keras\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ENTAILMENT():\n",
    "\n",
    "    def __init__(self) : \n",
    "        S = Sentence_ent()\n",
    "        S.load_dep()\n",
    "        self.sentence = S\n",
    "\n",
    "    def load_model_malstm(self):\n",
    "        self.malstm_model = keras.models.load_model('MALSTM.h5')\n",
    "\n",
    "    def get_parameters(self,sentence_A_processed,sentence_B_processed):\n",
    "        features_function = K.function([left_input,right_input], [left_output,right_output])\n",
    "        features = features_function([sentence_A_processed,sentence_B_processed])\n",
    "        np_feature_difference = np.array(features)\n",
    "        left_matrix = np_feature_difference[0]\n",
    "        right_matrix = np_feature_difference[1]\n",
    "        diff_features= np.abs(np.subtract(left_matrix, right_matrix))\n",
    "        prod = np.multiply(left_matrix,right_matrix)\n",
    "        self.extracted_features = np.concatenate([diff_features,prod], axis = 1)\n",
    "\n",
    "    def load_model_entailment(self):\n",
    "        self.dense_model = keras.models.load_model('CONTRADICTION_PREDICTION.h5')\n",
    "\n",
    "    def entailment_prediction(self):\n",
    "        pred_val =np.argmax(self.dense_model.predict(self.extracted_features))\n",
    "        return pred_val\n",
    "\n",
    "    def predict_value(self, A, B):\n",
    "#         \"CONTRADICTION\" : 0 , \"ENTAILMENT\" : 1 ,\"NEUTRAL\" : 2\n",
    "        S = self.sentence\n",
    "\n",
    "        S.sentence = A\n",
    "        A = S.preprocess()\n",
    "        S.sentence = B\n",
    "        B = S.preprocess()\n",
    "\n",
    "        sentence_A_processed = A\n",
    "        sentence_B_processed = B \n",
    "        self.load_model_malstm()\n",
    "        self.get_parameters(sentence_A_processed,sentence_B_processed)\n",
    "        self.load_model_entailment()\n",
    "        entailment = self.entailment_prediction()\n",
    "        return entailment\n",
    "\n",
    "class Sentence_ent():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab_location = 'sick_vocab.pickle'\n",
    "        self.embeddings_location = 'sick_embedding.pickle'\n",
    "        # self.word2vec_location = '/Users/ebby/Documents/Fake news/MALSTM/GoogleNews-vectors-negative300.bin'\n",
    "        \n",
    "    def preprocess(self):\n",
    "        text = self.sentence\n",
    "        text = str(text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "        text = re.sub(r\"what's\", \"what is \", text)\n",
    "        text = re.sub(r\"\\'s\", \" \", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "        text = re.sub(r\"can't\", \"cannot \", text)\n",
    "        text = re.sub(r\"n't\", \" not \", text)\n",
    "        text = re.sub(r\"i'm\", \"i am \", text)\n",
    "        text = re.sub(r\"\\'re\", \" are \", text)\n",
    "        text = re.sub(r\"\\'d\", \" would \", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "        text = re.sub(r\",\", \" \", text)\n",
    "        text = re.sub(r\"\\.\", \" \", text)\n",
    "        text = re.sub(r\"!\", \" ! \", text)\n",
    "        text = re.sub(r\"\\/\", \" \", text)\n",
    "        text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "        text = re.sub(r\"\\+\", \" + \", text)\n",
    "        text = re.sub(r\"\\-\", \" - \", text)\n",
    "        text = re.sub(r\"\\=\", \" = \", text)\n",
    "        text = re.sub(r\"'\", \" \", text)\n",
    "        text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "        text = re.sub(r\":\", \" : \", text)\n",
    "        text = re.sub(r\" e g \", \" eg \", text)\n",
    "        text = re.sub(r\" b g \", \" bg \", text)\n",
    "        text = re.sub(r\" u s \", \" american \", text)\n",
    "        text = re.sub(r\"\\0s\", \"0\", text)\n",
    "        text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "        text = re.sub(r\"e - mail\", \"email\", text)\n",
    "        text = re.sub(r\"j k\", \"jk\", text)\n",
    "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "        text = text.split()\n",
    "        self.word_list = text\n",
    "        self.to_index_and_pad()\n",
    "        \n",
    "        return self.index_rep\n",
    "        \n",
    "    def load_dep(self) : \n",
    "        file = open(self.vocab_location,\"rb\")\n",
    "        self.vocabulary = pickle.load(file)\n",
    "        file.close()\n",
    "        file = open(self.embeddings_location, \"rb\")\n",
    "        self.embeddings = pickle.load(file)\n",
    "        file.close()\n",
    "        print(\"Dependencies loaded!\")\n",
    "        \n",
    "    def to_index_and_pad(self) : \n",
    "        \n",
    "        self.index_rep = []\n",
    "        for word in self.word_list : \n",
    "            if word in self.vocabulary : \n",
    "                self.index_rep.append(self.vocabulary[word])\n",
    "        self.index_rep = pad_sequences([self.index_rep], maxlen=26)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = \"the dog was barking\"\n",
    "B = \"the dog was running in the park\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded!\n"
     ]
    }
   ],
   "source": [
    "e = ENTAILMENT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Input, Embedding, LSTM, Merge, Dense,Concatenate,merge, multiply\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contradiction\n"
     ]
    }
   ],
   "source": [
    "print(\"Contradiction\")\n",
    "data = pd.read_csv('../main/SICK.txt', sep=\"\\t\", header=None)\n",
    "data.columns = data.iloc[0]\n",
    "data = data.reindex(data.index.drop(0))\n",
    "\n",
    "EMBEDDING_FILE = '../main/GoogleNews-vectors-negative300.bin'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec loaded!\n"
     ]
    }
   ],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "#test_df = #pd.read_csv('test file location', sep=\"\\t\", header=None)\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']  \n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "questions_cols = ['sentence_A' , 'sentence_B']\n",
    "\n",
    "print(\"word2vec loaded!\")\n",
    "\n",
    "for dataset in [data]:\n",
    "    for index, row in dataset.iterrows():\n",
    "        for question in questions_cols:\n",
    "\n",
    "            q2n = []  \n",
    "            for word in text_to_word_list(row[question]):\n",
    "\n",
    "                 # Check for unwanted words\n",
    "                if word in stops and word not in word2vec.vocab:\n",
    "                    continue\n",
    "\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(inverse_vocabulary)\n",
    "                    q2n.append(len(inverse_vocabulary))\n",
    "                    inverse_vocabulary.append(word)\n",
    "                    #print(word)\n",
    "                else:\n",
    "                    q2n.append(vocabulary[word])\n",
    "            dataset.set_value(index, question, q2n)\n",
    "\n",
    "embedding_dim = 300\n",
    "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "embeddings[0] = 0 \n",
    "for word, index in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embeddings[index] = word2vec.word_vec(word)\n",
    "\n",
    "del word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions_cols = ['sentence_A' , 'sentence_B']\n",
    "X_train = data[questions_cols]\n",
    "Y_train = data['relatedness_score']\n",
    "X_train = {'left': X_train.sentence_A, 'right': X_train.sentence_B}\n",
    "Y_train = Y_train.values\n",
    "max_seq_length = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset, side in itertools.product([X_train], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 50\n",
    "gradient_clipping_norm = 1\n",
    "batch_size = 64\n",
    "n_epoch = 25\n",
    "print(\"model 1\")\n",
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    return (K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True)))*4.0 +1\n",
    "\n",
    "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "shared_lstm = LSTM(n_hidden)\n",
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)\n",
    "malstm_distance = Merge(mode=lambda x: ((K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True)))*4.0 +1), output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "malstm = Model([left_input, right_input], [malstm_distance])\n",
    "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
    "malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "9840/9840 [==============================] - 22s 2ms/step - loss: 1.6069 - mean_squared_error: 1.6069\n",
      "Epoch 2/25\n",
      "9840/9840 [==============================] - 21s 2ms/step - loss: 0.6733 - mean_squared_error: 0.6733\n",
      "Epoch 3/25\n",
      "9840/9840 [==============================] - 20s 2ms/step - loss: 0.5094 - mean_squared_error: 0.5094\n",
      "Epoch 4/25\n",
      "9840/9840 [==============================] - 20s 2ms/step - loss: 0.4355 - mean_squared_error: 0.4355\n",
      "Epoch 5/25\n",
      "9840/9840 [==============================] - 19s 2ms/step - loss: 0.3889 - mean_squared_error: 0.3889\n",
      "Epoch 6/25\n",
      "9840/9840 [==============================] - 19s 2ms/step - loss: 0.3564 - mean_squared_error: 0.3564\n",
      "Epoch 7/25\n",
      "9840/9840 [==============================] - 19s 2ms/step - loss: 0.3327 - mean_squared_error: 0.3327\n",
      "Epoch 8/25\n",
      "9840/9840 [==============================] - 19s 2ms/step - loss: 0.3144 - mean_squared_error: 0.3144\n",
      "Epoch 9/25\n",
      "9840/9840 [==============================] - 18s 2ms/step - loss: 0.2983 - mean_squared_error: 0.2983\n",
      "Epoch 10/25\n",
      "9840/9840 [==============================] - 18s 2ms/step - loss: 0.2848 - mean_squared_error: 0.2848\n",
      "Epoch 11/25\n",
      "9840/9840 [==============================] - 19s 2ms/step - loss: 0.2730 - mean_squared_error: 0.2730\n",
      "Epoch 12/25\n",
      "9840/9840 [==============================] - 19s 2ms/step - loss: 0.2635 - mean_squared_error: 0.2635\n",
      "Epoch 13/25\n",
      "9840/9840 [==============================] - 19s 2ms/step - loss: 0.2549 - mean_squared_error: 0.2549\n",
      "Epoch 14/25\n",
      "9840/9840 [==============================] - 18s 2ms/step - loss: 0.2480 - mean_squared_error: 0.2480\n",
      "Epoch 15/25\n",
      "9840/9840 [==============================] - 19s 2ms/step - loss: 0.2415 - mean_squared_error: 0.2415\n",
      "Epoch 16/25\n",
      "9840/9840 [==============================] - 21s 2ms/step - loss: 0.2354 - mean_squared_error: 0.2354\n",
      "Epoch 17/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.2305 - mean_squared_error: 0.2305\n",
      "Epoch 18/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.2254 - mean_squared_error: 0.2254\n",
      "Epoch 19/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.2210 - mean_squared_error: 0.2210\n",
      "Epoch 20/25\n",
      "9840/9840 [==============================] - 17s 2ms/step - loss: 0.2165 - mean_squared_error: 0.2165\n",
      "Epoch 21/25\n",
      "9840/9840 [==============================] - 18s 2ms/step - loss: 0.2126 - mean_squared_error: 0.2126\n",
      "Epoch 22/25\n",
      "9840/9840 [==============================] - 19s 2ms/step - loss: 0.2084 - mean_squared_error: 0.2084\n",
      "Epoch 23/25\n",
      "9840/9840 [==============================] - 18s 2ms/step - loss: 0.2060 - mean_squared_error: 0.2060\n",
      "Epoch 24/25\n",
      "9840/9840 [==============================] - 18s 2ms/step - loss: 0.2022 - mean_squared_error: 0.2022\n",
      "Epoch 25/25\n",
      "9840/9840 [==============================] - 19s 2ms/step - loss: 0.1989 - mean_squared_error: 0.1989\n",
      "Training time finished.\n",
      "25 epochs in 0:07:51.089037\n"
     ]
    }
   ],
   "source": [
    "training_start_time = time()\n",
    "malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch)\n",
    "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 26)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 26)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 26, 300)      691200      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 50)           70200       embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Merge)                 (None, 1)            0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 761,400\n",
      "Trainable params: 70,200\n",
      "Non-trainable params: 691,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "malstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_1:0' shape=(?, 26) dtype=int32>,\n",
       " <tf.Tensor 'input_2:0' shape=(?, 26) dtype=int32>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malstm.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Layer lstm_1 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-52b6dde35f58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmalstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmalstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36moutput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             raise AttributeError('Layer ' + self.name +\n\u001b[0;32m--> 955\u001b[0;31m                                  \u001b[0;34m' has multiple inbound nodes, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m                                  \u001b[0;34m'hence the notion of \"layer output\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m                                  \u001b[0;34m'is ill-defined. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Layer lstm_1 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead."
     ]
    }
   ],
   "source": [
    "l3 = K.function([malstm.layers[0].input, K.learning_phase()], [malstm.layers[3].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intermediate_layer_model = Model(inputs=[malstm.input[0]], outputs=[malstm.layers[4].get_input_at(0)[0]])\n",
    "# intermediate_output = intermediate_layer_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-5bf15527f665>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmalstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lstm_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'output'"
     ]
    }
   ],
   "source": [
    "malstm.get_layer('lstm_1').get_input_at(0).output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_2:0' shape=(?, 26) dtype=int32>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malstm.input[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 27, 28,  3,\n",
       "       35, 30, 31,  5, 32, 11,  9,  5, 34], dtype=int32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"right\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left = dataset[\"left\"][10:15]\n",
    "right = dataset[\"right\"][10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intermediate_output = intermediate_layer_model.predict([left])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-01f6903bbec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmalstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "malstm.layers[4].get_input_at(0)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 26)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 27, 28,  3,\n",
       "       35, 30, 31,  5, 32, 11,  9,  5, 34], dtype=int32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.77754222e-03,   2.14841729e-03,   1.91244471e-05,\n",
       "         -1.94815442e-01,  -2.77200460e-01,   3.04105133e-01,\n",
       "          2.64453322e-01,   5.65008521e-02,   1.28314563e-03,\n",
       "         -1.74586892e-01,   1.92945302e-01,   5.40861845e-01,\n",
       "         -8.18348853e-05,   5.82041964e-02,  -4.72569019e-02,\n",
       "          2.68744737e-01,  -5.25615104e-02,  -7.28903860e-02,\n",
       "          3.28205675e-01,   2.23158300e-01,  -3.81865129e-02,\n",
       "         -4.95558558e-03,  -9.65355357e-05,  -6.66834693e-03,\n",
       "          1.67123362e-01,  -4.07805890e-01,   3.63875516e-02,\n",
       "          7.88677542e-04,   1.32974043e-01,   4.33675665e-03,\n",
       "          1.30774617e-01,  -1.38347282e-03,   4.17650212e-03,\n",
       "         -3.93529087e-01,   2.44918928e-01,  -1.27814621e-01,\n",
       "         -1.87294915e-01,  -1.85248524e-01,  -4.95179780e-02,\n",
       "          2.55396720e-02,   3.44460859e-04,  -0.00000000e+00,\n",
       "         -1.87056642e-02,   2.82574862e-01,  -4.14009318e-02,\n",
       "          5.48415817e-03,   6.80437486e-04,   4.57161099e-01,\n",
       "         -1.50366917e-01,   0.00000000e+00],\n",
       "       [  1.00338291e-02,  -1.75524081e-04,  -6.04012748e-03,\n",
       "         -2.28345439e-01,  -2.37094074e-01,   2.65195101e-01,\n",
       "          2.50798494e-01,   5.65835461e-02,  -2.82067363e-03,\n",
       "         -1.47285163e-01,   2.33548313e-01,   5.30349255e-01,\n",
       "         -1.73316555e-04,   4.07010466e-02,  -7.35778138e-02,\n",
       "          1.91443637e-01,  -3.23813185e-02,  -8.63496438e-02,\n",
       "          3.60969752e-01,   2.42487460e-01,  -4.95163351e-02,\n",
       "         -1.09967054e-03,  -1.30093205e-04,  -8.65766127e-03,\n",
       "          1.45180032e-01,  -3.61125201e-01,   3.04595139e-02,\n",
       "          3.62272840e-04,   1.17822558e-01,  -2.32072710e-03,\n",
       "          9.89574343e-02,  -3.77166132e-03,  -1.61085685e-03,\n",
       "         -4.07388180e-01,   2.73545533e-01,  -1.38375923e-01,\n",
       "         -1.67507946e-01,  -1.55467480e-01,  -4.00632471e-02,\n",
       "          2.26402711e-02,   2.24727229e-03,   3.38505779e-05,\n",
       "         -2.02356745e-02,   2.60043412e-01,  -4.71328348e-02,\n",
       "          5.60073275e-03,  -2.34766887e-03,   4.57541019e-01,\n",
       "         -1.77562341e-01,   0.00000000e+00],\n",
       "       [  1.41166626e-02,   8.17097243e-05,  -5.42947976e-03,\n",
       "         -2.22235188e-01,  -2.39419609e-01,   2.88022786e-01,\n",
       "          2.52488226e-01,   3.27352211e-02,   0.00000000e+00,\n",
       "         -1.46663442e-01,   2.62334824e-01,   5.36833465e-01,\n",
       "          4.56432936e-05,   4.09209244e-02,  -7.75120035e-02,\n",
       "          1.85138285e-01,  -3.15399282e-02,  -1.00903206e-01,\n",
       "          3.40236217e-01,   2.33314306e-01,  -5.36747351e-02,\n",
       "         -4.36200527e-03,  -3.34526936e-04,  -9.03727487e-03,\n",
       "          1.46537051e-01,  -3.61456364e-01,   3.04359347e-02,\n",
       "          1.29209307e-03,   1.32901743e-01,  -2.75541621e-04,\n",
       "          9.61579308e-02,  -5.36005152e-03,  -2.38977582e-03,\n",
       "         -3.85687619e-01,   3.02712381e-01,  -1.23725578e-01,\n",
       "         -2.14629620e-01,  -1.72362760e-01,  -3.90317254e-02,\n",
       "          3.33053172e-02,   2.08674930e-03,   0.00000000e+00,\n",
       "         -1.68040637e-02,   2.52492458e-01,  -3.28636244e-02,\n",
       "          5.39785204e-03,  -2.07886030e-03,   4.39806074e-01,\n",
       "         -1.72622129e-01,  -0.00000000e+00],\n",
       "       [ -2.77754222e-03,   2.14841729e-03,   1.91244471e-05,\n",
       "         -1.94815442e-01,  -2.77200460e-01,   3.04105133e-01,\n",
       "          2.64453322e-01,   5.65008521e-02,   1.28314563e-03,\n",
       "         -1.74586892e-01,   1.92945302e-01,   5.40861845e-01,\n",
       "         -8.18348853e-05,   5.82041964e-02,  -4.72569019e-02,\n",
       "          2.68744737e-01,  -5.25615104e-02,  -7.28903860e-02,\n",
       "          3.28205675e-01,   2.23158300e-01,  -3.81865129e-02,\n",
       "         -4.95558558e-03,  -9.65355357e-05,  -6.66834693e-03,\n",
       "          1.67123362e-01,  -4.07805890e-01,   3.63875516e-02,\n",
       "          7.88677542e-04,   1.32974043e-01,   4.33675665e-03,\n",
       "          1.30774617e-01,  -1.38347282e-03,   4.17650212e-03,\n",
       "         -3.93529087e-01,   2.44918928e-01,  -1.27814621e-01,\n",
       "         -1.87294915e-01,  -1.85248524e-01,  -4.95179780e-02,\n",
       "          2.55396720e-02,   3.44460859e-04,  -0.00000000e+00,\n",
       "         -1.87056642e-02,   2.82574862e-01,  -4.14009318e-02,\n",
       "          5.48415817e-03,   6.80437486e-04,   4.57161099e-01,\n",
       "         -1.50366917e-01,   0.00000000e+00],\n",
       "       [ -2.77753943e-03,   2.14841729e-03,   1.91242798e-05,\n",
       "         -1.94815442e-01,  -2.77200460e-01,   3.04105133e-01,\n",
       "          2.64453322e-01,   5.65008633e-02,   1.28314423e-03,\n",
       "         -1.74586877e-01,   1.92945257e-01,   5.40861845e-01,\n",
       "         -8.18348708e-05,   5.82041964e-02,  -4.72569019e-02,\n",
       "          2.68744737e-01,  -5.25615141e-02,  -7.28903785e-02,\n",
       "          3.28205675e-01,   2.23158300e-01,  -3.81865129e-02,\n",
       "         -4.95558511e-03,  -9.65354557e-05,  -6.66834693e-03,\n",
       "          1.67123362e-01,  -4.07805920e-01,   3.63875516e-02,\n",
       "          7.88677426e-04,   1.32974043e-01,   4.33675619e-03,\n",
       "          1.30774617e-01,  -1.38347165e-03,   4.17650118e-03,\n",
       "         -3.93529087e-01,   2.44918928e-01,  -1.27814621e-01,\n",
       "         -1.87294915e-01,  -1.85248524e-01,  -4.95179780e-02,\n",
       "          2.55396720e-02,   3.44460714e-04,  -0.00000000e+00,\n",
       "         -1.87056661e-02,   2.82574862e-01,  -4.14009318e-02,\n",
       "          5.48415678e-03,   6.80437312e-04,   4.57161099e-01,\n",
       "         -1.50366917e-01,   0.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 26)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 26, 300)           691200    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                70200     \n",
      "=================================================================\n",
      "Total params: 761,400\n",
      "Trainable params: 70,200\n",
      "Non-trainable params: 691,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "intermediate_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.topology.InputLayer at 0x1f3a449b0>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malstm.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# serialize model to JSON\n",
    "model_json = intermediate_layer_model.to_json()\n",
    "with open(\"malstm.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "intermediate_layer_model.save_weights(\"malstm_weights.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Input, Embedding, LSTM, Merge,merge,Dense,Dropout\n",
    "from keras.optimizers import Adadelta,Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('malstm.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"malstm_weights.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 26)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 26, 300)           691200    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                70200     \n",
      "=================================================================\n",
      "Total params: 761,400\n",
      "Trainable params: 70,200\n",
      "Non-trainable params: 691,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loaded_model.predict([left])\n",
    "left_matrix = intermediate_layer_model.predict([dataset['left']])\n",
    "right_matrix = intermediate_layer_model.predict(dataset['right'])\n",
    "diff_features = np.abs(np.subtract(left_matrix, right_matrix))\n",
    "prod = np.multiply(left_matrix,right_matrix)\n",
    "extracted_features = np.concatenate([diff_features,prod], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9840, 100)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(200, input_shape=(100,),kernel_initializer='he_normal', activation='relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(100, kernel_initializer='he_normal', activation='relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(50, kernel_initializer='he_normal', activation='relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(3, activation='softmax'))\n",
    "model2.compile(Adam(lr = 0.001),'categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(extracted_features, y_TRAIN_cat.values,batch_size=16,epochs=10,verbose=1)\n",
    "# print(\"Entailmet model finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = intermediate_layer_model.predict(dataset[\"left\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = intermediate_layer_model.predict(dataset[\"right\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9840"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2  = [n(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(l[0]) + list(l[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(l)) : \n",
    "    i2 += [list(l[i]) + list(r[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(i2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train1 = data['entailment_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y2 = pd.get_dummies(Y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_102 (Dense)            (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 45,503\n",
      "Trainable params: 45,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list(y2.values[0])\n",
    "i3 = np.array(i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9840, 100)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9840/9840 [==============================] - 5s 495us/step - loss: 0.5724 - acc: 0.7679\n",
      "Epoch 2/10\n",
      "9840/9840 [==============================] - 3s 352us/step - loss: 0.4081 - acc: 0.8486\n",
      "Epoch 3/10\n",
      "9840/9840 [==============================] - 3s 351us/step - loss: 0.3937 - acc: 0.8568\n",
      "Epoch 4/10\n",
      "9840/9840 [==============================] - 4s 358us/step - loss: 0.3875 - acc: 0.8588\n",
      "Epoch 5/10\n",
      "9840/9840 [==============================] - 3s 348us/step - loss: 0.3866 - acc: 0.8589\n",
      "Epoch 6/10\n",
      "9840/9840 [==============================] - 3s 354us/step - loss: 0.3822 - acc: 0.8612\n",
      "Epoch 7/10\n",
      "9840/9840 [==============================] - 3s 350us/step - loss: 0.3808 - acc: 0.8610\n",
      "Epoch 8/10\n",
      "9840/9840 [==============================] - 3s 348us/step - loss: 0.3773 - acc: 0.8620\n",
      "Epoch 9/10\n",
      "9840/9840 [==============================] - 3s 355us/step - loss: 0.3720 - acc: 0.8620\n",
      "Epoch 10/10\n",
      "9840/9840 [==============================] - 3s 353us/step - loss: 0.3721 - acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1323e04a8>"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(extracted_features, y2.values,batch_size=16,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/90\n",
      "7379/7379 [==============================] - 4s 505us/step - loss: 0.9647 - acc: 0.5684\n",
      "Epoch 2/90\n",
      "7379/7379 [==============================] - 3s 342us/step - loss: 0.9424 - acc: 0.5689\n",
      "Epoch 3/90\n",
      "7379/7379 [==============================] - 3s 351us/step - loss: 0.9147 - acc: 0.5728\n",
      "Epoch 4/90\n",
      "7379/7379 [==============================] - 3s 339us/step - loss: 0.8911 - acc: 0.5776\n",
      "Epoch 5/90\n",
      "7379/7379 [==============================] - 3s 344us/step - loss: 0.8867 - acc: 0.5774\n",
      "Epoch 6/90\n",
      "7379/7379 [==============================] - 3s 351us/step - loss: 0.8726 - acc: 0.5810\n",
      "Epoch 7/90\n",
      "7379/7379 [==============================] - 3s 356us/step - loss: 0.8693 - acc: 0.5772\n",
      "Epoch 8/90\n",
      "7379/7379 [==============================] - 3s 342us/step - loss: 0.8590 - acc: 0.5840\n",
      "Epoch 9/90\n",
      "7379/7379 [==============================] - 3s 343us/step - loss: 0.8383 - acc: 0.5877\n",
      "Epoch 10/90\n",
      "7379/7379 [==============================] - 3s 345us/step - loss: 0.8159 - acc: 0.5934\n",
      "Epoch 11/90\n",
      "7379/7379 [==============================] - 3s 351us/step - loss: 0.7914 - acc: 0.6090\n",
      "Epoch 12/90\n",
      "7379/7379 [==============================] - 3s 352us/step - loss: 0.7768 - acc: 0.6154\n",
      "Epoch 13/90\n",
      "7379/7379 [==============================] - 3s 344us/step - loss: 0.7658 - acc: 0.6287\n",
      "Epoch 14/90\n",
      "7379/7379 [==============================] - 3s 352us/step - loss: 0.7529 - acc: 0.6284\n",
      "Epoch 15/90\n",
      "7379/7379 [==============================] - 2s 337us/step - loss: 0.7465 - acc: 0.6376\n",
      "Epoch 16/90\n",
      "7379/7379 [==============================] - 3s 352us/step - loss: 0.7346 - acc: 0.6550\n",
      "Epoch 17/90\n",
      "7379/7379 [==============================] - 3s 340us/step - loss: 0.7335 - acc: 0.6467\n",
      "Epoch 18/90\n",
      "7379/7379 [==============================] - 3s 349us/step - loss: 0.7291 - acc: 0.6523\n",
      "Epoch 19/90\n",
      "7379/7379 [==============================] - 3s 353us/step - loss: 0.7155 - acc: 0.6642\n",
      "Epoch 20/90\n",
      "7379/7379 [==============================] - 3s 352us/step - loss: 0.7085 - acc: 0.6680\n",
      "Epoch 21/90\n",
      "7379/7379 [==============================] - 3s 349us/step - loss: 0.7108 - acc: 0.6658\n",
      "Epoch 22/90\n",
      "7379/7379 [==============================] - 3s 348us/step - loss: 0.7066 - acc: 0.6693\n",
      "Epoch 23/90\n",
      "7379/7379 [==============================] - 3s 352us/step - loss: 0.7090 - acc: 0.6658\n",
      "Epoch 24/90\n",
      "7379/7379 [==============================] - 3s 350us/step - loss: 0.7055 - acc: 0.6696\n",
      "Epoch 25/90\n",
      "7379/7379 [==============================] - 3s 341us/step - loss: 0.6951 - acc: 0.6745\n",
      "Epoch 26/90\n",
      "7379/7379 [==============================] - 3s 347us/step - loss: 0.7016 - acc: 0.6800\n",
      "Epoch 27/90\n",
      "7379/7379 [==============================] - 3s 352us/step - loss: 0.7014 - acc: 0.6777\n",
      "Epoch 28/90\n",
      "7379/7379 [==============================] - 3s 345us/step - loss: 0.6867 - acc: 0.6863\n",
      "Epoch 29/90\n",
      "7379/7379 [==============================] - 3s 349us/step - loss: 0.6808 - acc: 0.6886\n",
      "Epoch 30/90\n",
      "7379/7379 [==============================] - 3s 352us/step - loss: 0.6816 - acc: 0.6869\n",
      "Epoch 31/90\n",
      "7379/7379 [==============================] - 3s 355us/step - loss: 0.6745 - acc: 0.6880\n",
      "Epoch 32/90\n",
      "7379/7379 [==============================] - 3s 343us/step - loss: 0.6757 - acc: 0.6907\n",
      "Epoch 33/90\n",
      "7379/7379 [==============================] - 2s 335us/step - loss: 0.6744 - acc: 0.6856\n",
      "Epoch 34/90\n",
      "7379/7379 [==============================] - 3s 354us/step - loss: 0.6632 - acc: 0.6977\n",
      "Epoch 35/90\n",
      "7379/7379 [==============================] - 3s 350us/step - loss: 0.6673 - acc: 0.6922\n",
      "Epoch 36/90\n",
      "7379/7379 [==============================] - 2s 339us/step - loss: 0.6502 - acc: 0.7066\n",
      "Epoch 37/90\n",
      "7379/7379 [==============================] - 3s 345us/step - loss: 0.6609 - acc: 0.6994\n",
      "Epoch 38/90\n",
      "7379/7379 [==============================] - 3s 346us/step - loss: 0.6480 - acc: 0.7075\n",
      "Epoch 39/90\n",
      "7379/7379 [==============================] - 3s 351us/step - loss: 0.6375 - acc: 0.7128\n",
      "Epoch 40/90\n",
      "7379/7379 [==============================] - 3s 355us/step - loss: 0.6421 - acc: 0.7165\n",
      "Epoch 41/90\n",
      "7379/7379 [==============================] - 2s 334us/step - loss: 0.6265 - acc: 0.7177\n",
      "Epoch 42/90\n",
      "7379/7379 [==============================] - 3s 372us/step - loss: 0.6400 - acc: 0.7097\n",
      "Epoch 43/90\n",
      "7379/7379 [==============================] - 3s 390us/step - loss: 0.6277 - acc: 0.7173\n",
      "Epoch 44/90\n",
      "7379/7379 [==============================] - 3s 361us/step - loss: 0.6226 - acc: 0.7206\n",
      "Epoch 45/90\n",
      "7379/7379 [==============================] - 3s 357us/step - loss: 0.6057 - acc: 0.7276\n",
      "Epoch 46/90\n",
      "7379/7379 [==============================] - 3s 349us/step - loss: 0.6093 - acc: 0.7207\n",
      "Epoch 47/90\n",
      "7379/7379 [==============================] - 3s 350us/step - loss: 0.6276 - acc: 0.7123\n",
      "Epoch 48/90\n",
      "7379/7379 [==============================] - 3s 347us/step - loss: 0.5973 - acc: 0.7365\n",
      "Epoch 49/90\n",
      "7379/7379 [==============================] - 3s 347us/step - loss: 0.5970 - acc: 0.7328\n",
      "Epoch 50/90\n",
      "7379/7379 [==============================] - 3s 354us/step - loss: 0.5968 - acc: 0.7340\n",
      "Epoch 51/90\n",
      "7379/7379 [==============================] - 3s 345us/step - loss: 0.5958 - acc: 0.7277\n",
      "Epoch 52/90\n",
      "7379/7379 [==============================] - 3s 339us/step - loss: 0.5860 - acc: 0.7399\n",
      "Epoch 53/90\n",
      "7379/7379 [==============================] - 3s 355us/step - loss: 0.5876 - acc: 0.7361\n",
      "Epoch 54/90\n",
      "7379/7379 [==============================] - 3s 357us/step - loss: 0.5764 - acc: 0.7475\n",
      "Epoch 55/90\n",
      "7379/7379 [==============================] - 3s 347us/step - loss: 0.5783 - acc: 0.7459\n",
      "Epoch 56/90\n",
      "7379/7379 [==============================] - 3s 344us/step - loss: 0.5783 - acc: 0.7447\n",
      "Epoch 57/90\n",
      "7379/7379 [==============================] - 3s 341us/step - loss: 0.5802 - acc: 0.7452\n",
      "Epoch 58/90\n",
      "7379/7379 [==============================] - 3s 342us/step - loss: 0.5716 - acc: 0.7448\n",
      "Epoch 59/90\n",
      "7379/7379 [==============================] - 3s 354us/step - loss: 0.5694 - acc: 0.7481\n",
      "Epoch 60/90\n",
      "7379/7379 [==============================] - 3s 352us/step - loss: 0.5757 - acc: 0.7454\n",
      "Epoch 61/90\n",
      "7379/7379 [==============================] - 3s 350us/step - loss: 0.5666 - acc: 0.7490\n",
      "Epoch 62/90\n",
      "7379/7379 [==============================] - 3s 355us/step - loss: 0.5717 - acc: 0.7445\n",
      "Epoch 63/90\n",
      "7379/7379 [==============================] - 3s 354us/step - loss: 0.5704 - acc: 0.7450\n",
      "Epoch 64/90\n",
      "7379/7379 [==============================] - 3s 366us/step - loss: 0.5676 - acc: 0.7452\n",
      "Epoch 65/90\n",
      "7379/7379 [==============================] - 3s 350us/step - loss: 0.5565 - acc: 0.7538\n",
      "Epoch 66/90\n",
      "7379/7379 [==============================] - 3s 351us/step - loss: 0.5575 - acc: 0.7558\n",
      "Epoch 67/90\n",
      "7379/7379 [==============================] - 3s 351us/step - loss: 0.5652 - acc: 0.7512\n",
      "Epoch 68/90\n",
      "7379/7379 [==============================] - 3s 350us/step - loss: 0.5543 - acc: 0.7590\n",
      "Epoch 69/90\n",
      "7379/7379 [==============================] - 3s 355us/step - loss: 0.5516 - acc: 0.7603\n",
      "Epoch 70/90\n",
      "7379/7379 [==============================] - 3s 351us/step - loss: 0.5608 - acc: 0.7496\n",
      "Epoch 71/90\n",
      "7379/7379 [==============================] - 3s 353us/step - loss: 0.5621 - acc: 0.7511\n",
      "Epoch 72/90\n",
      "7379/7379 [==============================] - 3s 346us/step - loss: 0.5475 - acc: 0.7586\n",
      "Epoch 73/90\n",
      "7379/7379 [==============================] - 3s 340us/step - loss: 0.5464 - acc: 0.7630\n",
      "Epoch 74/90\n",
      "7379/7379 [==============================] - 3s 346us/step - loss: 0.5461 - acc: 0.7608\n",
      "Epoch 75/90\n",
      "7379/7379 [==============================] - 3s 354us/step - loss: 0.5528 - acc: 0.7620\n",
      "Epoch 76/90\n",
      "7379/7379 [==============================] - 3s 354us/step - loss: 0.5392 - acc: 0.7662\n",
      "Epoch 77/90\n",
      "7379/7379 [==============================] - 3s 349us/step - loss: 0.5461 - acc: 0.7599\n",
      "Epoch 78/90\n",
      "7379/7379 [==============================] - 3s 354us/step - loss: 0.5442 - acc: 0.7576\n",
      "Epoch 79/90\n",
      "7379/7379 [==============================] - 3s 371us/step - loss: 0.5460 - acc: 0.7630\n",
      "Epoch 80/90\n",
      "7379/7379 [==============================] - 3s 347us/step - loss: 0.5367 - acc: 0.7654\n",
      "Epoch 81/90\n",
      "7379/7379 [==============================] - 3s 342us/step - loss: 0.5369 - acc: 0.7710\n",
      "Epoch 82/90\n",
      "7379/7379 [==============================] - 3s 342us/step - loss: 0.5525 - acc: 0.7585\n",
      "Epoch 83/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7379/7379 [==============================] - 3s 348us/step - loss: 0.5506 - acc: 0.7577\n",
      "Epoch 84/90\n",
      "7379/7379 [==============================] - 3s 346us/step - loss: 0.5351 - acc: 0.7696\n",
      "Epoch 85/90\n",
      "7379/7379 [==============================] - 3s 343us/step - loss: 0.5466 - acc: 0.7585\n",
      "Epoch 86/90\n",
      "7379/7379 [==============================] - 3s 351us/step - loss: 0.5319 - acc: 0.7688\n",
      "Epoch 87/90\n",
      "7379/7379 [==============================] - 3s 345us/step - loss: 0.5308 - acc: 0.7672\n",
      "Epoch 88/90\n",
      "7379/7379 [==============================] - 3s 351us/step - loss: 0.5395 - acc: 0.7706\n",
      "Epoch 89/90\n",
      "7379/7379 [==============================] - 3s 348us/step - loss: 0.5367 - acc: 0.7668\n",
      "Epoch 90/90\n",
      "7379/7379 [==============================] - 3s 358us/step - loss: 0.5295 - acc: 0.7704\n",
      "acc: 77.85%\n",
      "Epoch 1/90\n",
      "7380/7380 [==============================] - 4s 521us/step - loss: 0.9682 - acc: 0.5667\n",
      "Epoch 2/90\n",
      "7380/7380 [==============================] - 3s 355us/step - loss: 0.9444 - acc: 0.5687\n",
      "Epoch 3/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.9161 - acc: 0.5680\n",
      "Epoch 4/90\n",
      "7380/7380 [==============================] - 3s 355us/step - loss: 0.8954 - acc: 0.5702\n",
      "Epoch 5/90\n",
      "7380/7380 [==============================] - 3s 349us/step - loss: 0.8790 - acc: 0.5743\n",
      "Epoch 6/90\n",
      "7380/7380 [==============================] - 3s 347us/step - loss: 0.8704 - acc: 0.5757\n",
      "Epoch 7/90\n",
      "7380/7380 [==============================] - 3s 348us/step - loss: 0.8524 - acc: 0.5770\n",
      "Epoch 8/90\n",
      "7380/7380 [==============================] - 3s 348us/step - loss: 0.8338 - acc: 0.5827\n",
      "Epoch 9/90\n",
      "7380/7380 [==============================] - 3s 351us/step - loss: 0.8011 - acc: 0.5940\n",
      "Epoch 10/90\n",
      "7380/7380 [==============================] - 3s 344us/step - loss: 0.7748 - acc: 0.6057\n",
      "Epoch 11/90\n",
      "7380/7380 [==============================] - 3s 351us/step - loss: 0.7606 - acc: 0.6206\n",
      "Epoch 12/90\n",
      "7380/7380 [==============================] - 3s 377us/step - loss: 0.7502 - acc: 0.6211\n",
      "Epoch 13/90\n",
      "7380/7380 [==============================] - 3s 371us/step - loss: 0.7424 - acc: 0.6327\n",
      "Epoch 14/90\n",
      "7380/7380 [==============================] - 3s 356us/step - loss: 0.7256 - acc: 0.6362\n",
      "Epoch 15/90\n",
      "7380/7380 [==============================] - 3s 347us/step - loss: 0.7163 - acc: 0.6420\n",
      "Epoch 16/90\n",
      "7380/7380 [==============================] - 3s 377us/step - loss: 0.7051 - acc: 0.6467\n",
      "Epoch 17/90\n",
      "7380/7380 [==============================] - 3s 370us/step - loss: 0.7087 - acc: 0.6438\n",
      "Epoch 18/90\n",
      "7380/7380 [==============================] - 3s 355us/step - loss: 0.7011 - acc: 0.6453\n",
      "Epoch 19/90\n",
      "7380/7380 [==============================] - 3s 340us/step - loss: 0.6854 - acc: 0.6644\n",
      "Epoch 20/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.6790 - acc: 0.6623\n",
      "Epoch 21/90\n",
      "7380/7380 [==============================] - 3s 351us/step - loss: 0.6755 - acc: 0.6672\n",
      "Epoch 22/90\n",
      "7380/7380 [==============================] - 3s 363us/step - loss: 0.6635 - acc: 0.6757\n",
      "Epoch 23/90\n",
      "7380/7380 [==============================] - 3s 349us/step - loss: 0.6699 - acc: 0.6766\n",
      "Epoch 24/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.6550 - acc: 0.6835\n",
      "Epoch 25/90\n",
      "7380/7380 [==============================] - 3s 356us/step - loss: 0.6443 - acc: 0.6939\n",
      "Epoch 26/90\n",
      "7380/7380 [==============================] - 3s 354us/step - loss: 0.6512 - acc: 0.6835\n",
      "Epoch 27/90\n",
      "7380/7380 [==============================] - 3s 353us/step - loss: 0.6555 - acc: 0.6814\n",
      "Epoch 28/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.6406 - acc: 0.6875\n",
      "Epoch 29/90\n",
      "7380/7380 [==============================] - 3s 344us/step - loss: 0.6374 - acc: 0.7007\n",
      "Epoch 30/90\n",
      "7380/7380 [==============================] - 3s 352us/step - loss: 0.6435 - acc: 0.6883\n",
      "Epoch 31/90\n",
      "7380/7380 [==============================] - 3s 344us/step - loss: 0.6373 - acc: 0.6927\n",
      "Epoch 32/90\n",
      "7380/7380 [==============================] - 3s 339us/step - loss: 0.6245 - acc: 0.6993\n",
      "Epoch 33/90\n",
      "7380/7380 [==============================] - 2s 338us/step - loss: 0.6404 - acc: 0.6961\n",
      "Epoch 34/90\n",
      "7380/7380 [==============================] - 3s 365us/step - loss: 0.6334 - acc: 0.7027\n",
      "Epoch 35/90\n",
      "7380/7380 [==============================] - 3s 375us/step - loss: 0.6449 - acc: 0.6957\n",
      "Epoch 36/90\n",
      "7380/7380 [==============================] - 3s 377us/step - loss: 0.6142 - acc: 0.7104\n",
      "Epoch 37/90\n",
      "7380/7380 [==============================] - 3s 365us/step - loss: 0.6294 - acc: 0.7046\n",
      "Epoch 38/90\n",
      "7380/7380 [==============================] - 3s 370us/step - loss: 0.6173 - acc: 0.7062\n",
      "Epoch 39/90\n",
      "7380/7380 [==============================] - 3s 359us/step - loss: 0.6180 - acc: 0.7137\n",
      "Epoch 40/90\n",
      "7380/7380 [==============================] - 3s 342us/step - loss: 0.6150 - acc: 0.7157\n",
      "Epoch 41/90\n",
      "7380/7380 [==============================] - 3s 347us/step - loss: 0.6042 - acc: 0.7240\n",
      "Epoch 42/90\n",
      "7380/7380 [==============================] - 3s 344us/step - loss: 0.6037 - acc: 0.7202\n",
      "Epoch 43/90\n",
      "7380/7380 [==============================] - 3s 351us/step - loss: 0.6024 - acc: 0.7221\n",
      "Epoch 44/90\n",
      "7380/7380 [==============================] - 3s 350us/step - loss: 0.6065 - acc: 0.7270\n",
      "Epoch 45/90\n",
      "7380/7380 [==============================] - 3s 346us/step - loss: 0.6033 - acc: 0.7232\n",
      "Epoch 46/90\n",
      "7380/7380 [==============================] - 3s 347us/step - loss: 0.6034 - acc: 0.7222\n",
      "Epoch 47/90\n",
      "7380/7380 [==============================] - 3s 340us/step - loss: 0.5921 - acc: 0.7343\n",
      "Epoch 48/90\n",
      "7380/7380 [==============================] - 3s 344us/step - loss: 0.5938 - acc: 0.7317\n",
      "Epoch 49/90\n",
      "7380/7380 [==============================] - 2s 332us/step - loss: 0.5860 - acc: 0.7318\n",
      "Epoch 50/90\n",
      "7380/7380 [==============================] - 3s 343us/step - loss: 0.5779 - acc: 0.7409\n",
      "Epoch 51/90\n",
      "7380/7380 [==============================] - 3s 346us/step - loss: 0.5894 - acc: 0.7272\n",
      "Epoch 52/90\n",
      "7380/7380 [==============================] - 3s 348us/step - loss: 0.5806 - acc: 0.7367\n",
      "Epoch 53/90\n",
      "7380/7380 [==============================] - 3s 343us/step - loss: 0.5800 - acc: 0.7335\n",
      "Epoch 54/90\n",
      "7380/7380 [==============================] - 3s 350us/step - loss: 0.5790 - acc: 0.7362\n",
      "Epoch 55/90\n",
      "7380/7380 [==============================] - 3s 350us/step - loss: 0.5855 - acc: 0.7352\n",
      "Epoch 56/90\n",
      "7380/7380 [==============================] - 3s 346us/step - loss: 0.5780 - acc: 0.7421\n",
      "Epoch 57/90\n",
      "7380/7380 [==============================] - 3s 342us/step - loss: 0.5766 - acc: 0.7442\n",
      "Epoch 58/90\n",
      "7380/7380 [==============================] - 2s 337us/step - loss: 0.5714 - acc: 0.7438\n",
      "Epoch 59/90\n",
      "7380/7380 [==============================] - 3s 350us/step - loss: 0.5719 - acc: 0.7465\n",
      "Epoch 60/90\n",
      "7380/7380 [==============================] - 3s 347us/step - loss: 0.5729 - acc: 0.7434\n",
      "Epoch 61/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.5685 - acc: 0.7421\n",
      "Epoch 62/90\n",
      "7380/7380 [==============================] - 3s 344us/step - loss: 0.5668 - acc: 0.7470\n",
      "Epoch 63/90\n",
      "7380/7380 [==============================] - 3s 343us/step - loss: 0.5621 - acc: 0.7496\n",
      "Epoch 64/90\n",
      "7380/7380 [==============================] - 3s 352us/step - loss: 0.5677 - acc: 0.7476\n",
      "Epoch 65/90\n",
      "7380/7380 [==============================] - 3s 346us/step - loss: 0.5696 - acc: 0.7440\n",
      "Epoch 66/90\n",
      "7380/7380 [==============================] - 3s 351us/step - loss: 0.5629 - acc: 0.7519\n",
      "Epoch 67/90\n",
      "7380/7380 [==============================] - 3s 349us/step - loss: 0.5712 - acc: 0.7439\n",
      "Epoch 68/90\n",
      "7380/7380 [==============================] - 3s 353us/step - loss: 0.5681 - acc: 0.7493\n",
      "Epoch 69/90\n",
      "7380/7380 [==============================] - 3s 349us/step - loss: 0.5587 - acc: 0.7527\n",
      "Epoch 70/90\n",
      "7380/7380 [==============================] - 3s 349us/step - loss: 0.5612 - acc: 0.7499\n",
      "Epoch 71/90\n",
      "7380/7380 [==============================] - 3s 346us/step - loss: 0.5628 - acc: 0.7493\n",
      "Epoch 72/90\n",
      "7380/7380 [==============================] - 3s 350us/step - loss: 0.5664 - acc: 0.7480\n",
      "Epoch 73/90\n",
      "7380/7380 [==============================] - 3s 351us/step - loss: 0.5632 - acc: 0.7541\n",
      "Epoch 74/90\n",
      "7380/7380 [==============================] - 3s 347us/step - loss: 0.5578 - acc: 0.7516\n",
      "Epoch 75/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7380/7380 [==============================] - 3s 342us/step - loss: 0.5677 - acc: 0.7485\n",
      "Epoch 76/90\n",
      "7380/7380 [==============================] - 3s 344us/step - loss: 0.5553 - acc: 0.7550\n",
      "Epoch 77/90\n",
      "7380/7380 [==============================] - 3s 352us/step - loss: 0.5425 - acc: 0.7686\n",
      "Epoch 78/90\n",
      "7380/7380 [==============================] - 3s 351us/step - loss: 0.5577 - acc: 0.7497\n",
      "Epoch 79/90\n",
      "7380/7380 [==============================] - 3s 362us/step - loss: 0.5555 - acc: 0.7595\n",
      "Epoch 80/90\n",
      "7380/7380 [==============================] - 3s 340us/step - loss: 0.5548 - acc: 0.7599\n",
      "Epoch 81/90\n",
      "7380/7380 [==============================] - 2s 336us/step - loss: 0.5464 - acc: 0.7637\n",
      "Epoch 82/90\n",
      "7380/7380 [==============================] - 3s 346us/step - loss: 0.5505 - acc: 0.7570\n",
      "Epoch 83/90\n",
      "7380/7380 [==============================] - 3s 341us/step - loss: 0.5548 - acc: 0.7583\n",
      "Epoch 84/90\n",
      "7380/7380 [==============================] - 3s 343us/step - loss: 0.5428 - acc: 0.7615\n",
      "Epoch 85/90\n",
      "7380/7380 [==============================] - 3s 347us/step - loss: 0.5557 - acc: 0.7514\n",
      "Epoch 86/90\n",
      "7380/7380 [==============================] - 2s 335us/step - loss: 0.5508 - acc: 0.7621\n",
      "Epoch 87/90\n",
      "7380/7380 [==============================] - 3s 348us/step - loss: 0.5424 - acc: 0.7612\n",
      "Epoch 88/90\n",
      "7380/7380 [==============================] - 3s 350us/step - loss: 0.5376 - acc: 0.7659\n",
      "Epoch 89/90\n",
      "7380/7380 [==============================] - 3s 348us/step - loss: 0.5431 - acc: 0.7661\n",
      "Epoch 90/90\n",
      "7380/7380 [==============================] - 3s 350us/step - loss: 0.5429 - acc: 0.7673\n",
      "acc: 75.12%\n",
      "Epoch 1/90\n",
      "7380/7380 [==============================] - 4s 502us/step - loss: 0.9704 - acc: 0.5660\n",
      "Epoch 2/90\n",
      "7380/7380 [==============================] - 3s 348us/step - loss: 0.9538 - acc: 0.5686\n",
      "Epoch 3/90\n",
      "7380/7380 [==============================] - 3s 344us/step - loss: 0.9326 - acc: 0.5688\n",
      "Epoch 4/90\n",
      "7380/7380 [==============================] - 3s 346us/step - loss: 0.9092 - acc: 0.5725\n",
      "Epoch 5/90\n",
      "7380/7380 [==============================] - 3s 350us/step - loss: 0.8905 - acc: 0.5801\n",
      "Epoch 6/90\n",
      "7380/7380 [==============================] - 3s 343us/step - loss: 0.8786 - acc: 0.5818\n",
      "Epoch 7/90\n",
      "7380/7380 [==============================] - 3s 349us/step - loss: 0.8597 - acc: 0.5864\n",
      "Epoch 8/90\n",
      "7380/7380 [==============================] - 3s 344us/step - loss: 0.8406 - acc: 0.5874\n",
      "Epoch 9/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.8182 - acc: 0.5962\n",
      "Epoch 10/90\n",
      "7380/7380 [==============================] - 3s 347us/step - loss: 0.7949 - acc: 0.6060\n",
      "Epoch 11/90\n",
      "7380/7380 [==============================] - 3s 352us/step - loss: 0.7818 - acc: 0.6173\n",
      "Epoch 12/90\n",
      "7380/7380 [==============================] - 3s 346us/step - loss: 0.7616 - acc: 0.6322\n",
      "Epoch 13/90\n",
      "7380/7380 [==============================] - 3s 349us/step - loss: 0.7520 - acc: 0.6301\n",
      "Epoch 14/90\n",
      "7380/7380 [==============================] - 3s 349us/step - loss: 0.7295 - acc: 0.6367\n",
      "Epoch 15/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.7185 - acc: 0.6492\n",
      "Epoch 16/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.7152 - acc: 0.6505\n",
      "Epoch 17/90\n",
      "7380/7380 [==============================] - 3s 352us/step - loss: 0.7217 - acc: 0.6514\n",
      "Epoch 18/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.6998 - acc: 0.6599\n",
      "Epoch 19/90\n",
      "7380/7380 [==============================] - 3s 352us/step - loss: 0.6925 - acc: 0.6550\n",
      "Epoch 20/90\n",
      "7380/7380 [==============================] - 3s 355us/step - loss: 0.6905 - acc: 0.6699\n",
      "Epoch 21/90\n",
      "7380/7380 [==============================] - 3s 352us/step - loss: 0.6787 - acc: 0.6770\n",
      "Epoch 22/90\n",
      "7380/7380 [==============================] - 3s 348us/step - loss: 0.6773 - acc: 0.6795\n",
      "Epoch 23/90\n",
      "7380/7380 [==============================] - 3s 351us/step - loss: 0.6689 - acc: 0.6858\n",
      "Epoch 24/90\n",
      "7380/7380 [==============================] - 3s 346us/step - loss: 0.6699 - acc: 0.6771\n",
      "Epoch 25/90\n",
      "7380/7380 [==============================] - 3s 344us/step - loss: 0.6582 - acc: 0.6931\n",
      "Epoch 26/90\n",
      "7380/7380 [==============================] - 3s 347us/step - loss: 0.6536 - acc: 0.6912\n",
      "Epoch 27/90\n",
      "7380/7380 [==============================] - 3s 340us/step - loss: 0.6523 - acc: 0.6915\n",
      "Epoch 28/90\n",
      "7380/7380 [==============================] - 2s 337us/step - loss: 0.6426 - acc: 0.6972\n",
      "Epoch 29/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.6406 - acc: 0.6969\n",
      "Epoch 30/90\n",
      "7380/7380 [==============================] - 3s 349us/step - loss: 0.6484 - acc: 0.6976\n",
      "Epoch 31/90\n",
      "7380/7380 [==============================] - 3s 346us/step - loss: 0.6393 - acc: 0.6942\n",
      "Epoch 32/90\n",
      "7380/7380 [==============================] - 3s 355us/step - loss: 0.6300 - acc: 0.7087\n",
      "Epoch 33/90\n",
      "7380/7380 [==============================] - 3s 351us/step - loss: 0.6284 - acc: 0.7092\n",
      "Epoch 34/90\n",
      "7380/7380 [==============================] - 3s 344us/step - loss: 0.6358 - acc: 0.6980\n",
      "Epoch 35/90\n",
      "7380/7380 [==============================] - 3s 346us/step - loss: 0.6230 - acc: 0.7107\n",
      "Epoch 36/90\n",
      "7380/7380 [==============================] - 3s 344us/step - loss: 0.6258 - acc: 0.7058\n",
      "Epoch 37/90\n",
      "7380/7380 [==============================] - 3s 349us/step - loss: 0.6051 - acc: 0.7201\n",
      "Epoch 38/90\n",
      "7380/7380 [==============================] - 3s 342us/step - loss: 0.6150 - acc: 0.7179\n",
      "Epoch 39/90\n",
      "7380/7380 [==============================] - 3s 355us/step - loss: 0.6018 - acc: 0.7214\n",
      "Epoch 40/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.5985 - acc: 0.7233\n",
      "Epoch 41/90\n",
      "7380/7380 [==============================] - 3s 351us/step - loss: 0.6135 - acc: 0.7145\n",
      "Epoch 42/90\n",
      "7380/7380 [==============================] - 3s 342us/step - loss: 0.5902 - acc: 0.7366\n",
      "Epoch 43/90\n",
      "7380/7380 [==============================] - 3s 344us/step - loss: 0.5977 - acc: 0.7301\n",
      "Epoch 44/90\n",
      "7380/7380 [==============================] - 3s 353us/step - loss: 0.6059 - acc: 0.7176\n",
      "Epoch 45/90\n",
      "7380/7380 [==============================] - 3s 350us/step - loss: 0.5901 - acc: 0.7355\n",
      "Epoch 46/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.6016 - acc: 0.7263\n",
      "Epoch 47/90\n",
      "7380/7380 [==============================] - 3s 353us/step - loss: 0.5864 - acc: 0.7359\n",
      "Epoch 48/90\n",
      "7380/7380 [==============================] - 3s 347us/step - loss: 0.5854 - acc: 0.7356\n",
      "Epoch 49/90\n",
      "7380/7380 [==============================] - 3s 348us/step - loss: 0.5902 - acc: 0.7274\n",
      "Epoch 50/90\n",
      "7380/7380 [==============================] - 3s 347us/step - loss: 0.5787 - acc: 0.7328\n",
      "Epoch 51/90\n",
      "7380/7380 [==============================] - 3s 353us/step - loss: 0.5843 - acc: 0.7358\n",
      "Epoch 52/90\n",
      "7380/7380 [==============================] - 3s 343us/step - loss: 0.5764 - acc: 0.7377\n",
      "Epoch 53/90\n",
      "7380/7380 [==============================] - 3s 353us/step - loss: 0.5806 - acc: 0.7350\n",
      "Epoch 54/90\n",
      "7380/7380 [==============================] - 3s 347us/step - loss: 0.5710 - acc: 0.7484\n",
      "Epoch 55/90\n",
      "7380/7380 [==============================] - 3s 348us/step - loss: 0.5882 - acc: 0.7358\n",
      "Epoch 56/90\n",
      "7380/7380 [==============================] - 3s 347us/step - loss: 0.5790 - acc: 0.7407\n",
      "Epoch 57/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.5734 - acc: 0.7431\n",
      "Epoch 58/90\n",
      "7380/7380 [==============================] - 3s 348us/step - loss: 0.5644 - acc: 0.7444\n",
      "Epoch 59/90\n",
      "7380/7380 [==============================] - 2s 337us/step - loss: 0.5644 - acc: 0.7424\n",
      "Epoch 60/90\n",
      "7380/7380 [==============================] - 3s 344us/step - loss: 0.5577 - acc: 0.7554\n",
      "Epoch 61/90\n",
      "7380/7380 [==============================] - 3s 350us/step - loss: 0.5634 - acc: 0.7446\n",
      "Epoch 62/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.5706 - acc: 0.7411\n",
      "Epoch 63/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.5693 - acc: 0.7435\n",
      "Epoch 64/90\n",
      "7380/7380 [==============================] - 3s 340us/step - loss: 0.5527 - acc: 0.7512\n",
      "Epoch 65/90\n",
      "7380/7380 [==============================] - 3s 339us/step - loss: 0.5594 - acc: 0.7520\n",
      "Epoch 66/90\n",
      "7380/7380 [==============================] - 3s 351us/step - loss: 0.5574 - acc: 0.7549\n",
      "Epoch 67/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7380/7380 [==============================] - 3s 346us/step - loss: 0.5622 - acc: 0.7501\n",
      "Epoch 68/90\n",
      "7380/7380 [==============================] - 3s 349us/step - loss: 0.5549 - acc: 0.7549\n",
      "Epoch 69/90\n",
      "7380/7380 [==============================] - 3s 350us/step - loss: 0.5633 - acc: 0.7554\n",
      "Epoch 70/90\n",
      "7380/7380 [==============================] - 3s 349us/step - loss: 0.5690 - acc: 0.7473\n",
      "Epoch 71/90\n",
      "7380/7380 [==============================] - 3s 352us/step - loss: 0.5519 - acc: 0.7592\n",
      "Epoch 72/90\n",
      "7380/7380 [==============================] - 3s 348us/step - loss: 0.5527 - acc: 0.7564\n",
      "Epoch 73/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.5456 - acc: 0.7623\n",
      "Epoch 74/90\n",
      "7380/7380 [==============================] - 3s 350us/step - loss: 0.5510 - acc: 0.7607\n",
      "Epoch 75/90\n",
      "7380/7380 [==============================] - 3s 349us/step - loss: 0.5458 - acc: 0.7576\n",
      "Epoch 76/90\n",
      "7380/7380 [==============================] - 3s 349us/step - loss: 0.5503 - acc: 0.7538\n",
      "Epoch 77/90\n",
      "7380/7380 [==============================] - 3s 361us/step - loss: 0.5434 - acc: 0.7617\n",
      "Epoch 78/90\n",
      "7380/7380 [==============================] - 3s 364us/step - loss: 0.5570 - acc: 0.7576\n",
      "Epoch 79/90\n",
      "7380/7380 [==============================] - 3s 355us/step - loss: 0.5442 - acc: 0.7596\n",
      "Epoch 80/90\n",
      "7380/7380 [==============================] - 2s 339us/step - loss: 0.5378 - acc: 0.7623\n",
      "Epoch 81/90\n",
      "7380/7380 [==============================] - 3s 344us/step - loss: 0.5345 - acc: 0.7650\n",
      "Epoch 82/90\n",
      "7380/7380 [==============================] - 3s 344us/step - loss: 0.5367 - acc: 0.7653\n",
      "Epoch 83/90\n",
      "7380/7380 [==============================] - 3s 346us/step - loss: 0.5390 - acc: 0.7641\n",
      "Epoch 84/90\n",
      "7380/7380 [==============================] - 3s 346us/step - loss: 0.5332 - acc: 0.7654\n",
      "Epoch 85/90\n",
      "7380/7380 [==============================] - 3s 345us/step - loss: 0.5311 - acc: 0.7688\n",
      "Epoch 86/90\n",
      "7380/7380 [==============================] - 3s 351us/step - loss: 0.5317 - acc: 0.7722\n",
      "Epoch 87/90\n",
      "7380/7380 [==============================] - 3s 354us/step - loss: 0.5233 - acc: 0.7778\n",
      "Epoch 88/90\n",
      "7380/7380 [==============================] - 3s 350us/step - loss: 0.5435 - acc: 0.7667\n",
      "Epoch 89/90\n",
      "7380/7380 [==============================] - 3s 346us/step - loss: 0.5217 - acc: 0.7711\n",
      "Epoch 90/90\n",
      "7380/7380 [==============================] - 3s 350us/step - loss: 0.5291 - acc: 0.7686\n",
      "acc: 80.69%\n",
      "Epoch 1/90\n",
      "7381/7381 [==============================] - 4s 495us/step - loss: 0.9698 - acc: 0.5640\n",
      "Epoch 2/90\n",
      "7381/7381 [==============================] - 2s 316us/step - loss: 0.9535 - acc: 0.5688\n",
      "Epoch 3/90\n",
      "7381/7381 [==============================] - 2s 321us/step - loss: 0.9364 - acc: 0.5688\n",
      "Epoch 4/90\n",
      "7381/7381 [==============================] - 2s 321us/step - loss: 0.9019 - acc: 0.5730\n",
      "Epoch 5/90\n",
      "7381/7381 [==============================] - 2s 327us/step - loss: 0.8828 - acc: 0.5784\n",
      "Epoch 6/90\n",
      "7381/7381 [==============================] - 2s 320us/step - loss: 0.8691 - acc: 0.5808\n",
      "Epoch 7/90\n",
      "7381/7381 [==============================] - 2s 320us/step - loss: 0.8494 - acc: 0.5860\n",
      "Epoch 8/90\n",
      "7381/7381 [==============================] - 2s 315us/step - loss: 0.8159 - acc: 0.5967\n",
      "Epoch 9/90\n",
      "7381/7381 [==============================] - 2s 324us/step - loss: 0.8012 - acc: 0.6036\n",
      "Epoch 10/90\n",
      "7381/7381 [==============================] - 2s 322us/step - loss: 0.7747 - acc: 0.6129\n",
      "Epoch 11/90\n",
      "7381/7381 [==============================] - 2s 323us/step - loss: 0.7718 - acc: 0.6155\n",
      "Epoch 12/90\n",
      "7381/7381 [==============================] - 2s 324us/step - loss: 0.7515 - acc: 0.6357\n",
      "Epoch 13/90\n",
      "7381/7381 [==============================] - 2s 323us/step - loss: 0.7577 - acc: 0.6254\n",
      "Epoch 14/90\n",
      "7381/7381 [==============================] - 2s 326us/step - loss: 0.7389 - acc: 0.6396\n",
      "Epoch 15/90\n",
      "7381/7381 [==============================] - 2s 323us/step - loss: 0.7349 - acc: 0.6411\n",
      "Epoch 16/90\n",
      "7381/7381 [==============================] - 2s 322us/step - loss: 0.7295 - acc: 0.6426\n",
      "Epoch 17/90\n",
      "7381/7381 [==============================] - 2s 324us/step - loss: 0.7354 - acc: 0.6463\n",
      "Epoch 18/90\n",
      "7381/7381 [==============================] - 2s 328us/step - loss: 0.7274 - acc: 0.6423\n",
      "Epoch 19/90\n",
      "7381/7381 [==============================] - 2s 328us/step - loss: 0.7021 - acc: 0.6706\n",
      "Epoch 20/90\n",
      "7381/7381 [==============================] - 2s 323us/step - loss: 0.7053 - acc: 0.6618\n",
      "Epoch 21/90\n",
      "7381/7381 [==============================] - 2s 320us/step - loss: 0.6985 - acc: 0.6644\n",
      "Epoch 22/90\n",
      "7381/7381 [==============================] - 2s 324us/step - loss: 0.7007 - acc: 0.6675\n",
      "Epoch 23/90\n",
      "7381/7381 [==============================] - 2s 324us/step - loss: 0.6885 - acc: 0.6725\n",
      "Epoch 24/90\n",
      "7381/7381 [==============================] - 2s 317us/step - loss: 0.6774 - acc: 0.6786\n",
      "Epoch 25/90\n",
      "7381/7381 [==============================] - 2s 328us/step - loss: 0.6773 - acc: 0.6755\n",
      "Epoch 26/90\n",
      "7381/7381 [==============================] - 2s 326us/step - loss: 0.6747 - acc: 0.6792\n",
      "Epoch 27/90\n",
      "7381/7381 [==============================] - 2s 324us/step - loss: 0.6805 - acc: 0.6790\n",
      "Epoch 28/90\n",
      "7381/7381 [==============================] - 2s 327us/step - loss: 0.6550 - acc: 0.6910\n",
      "Epoch 29/90\n",
      "7381/7381 [==============================] - 2s 329us/step - loss: 0.6634 - acc: 0.6809\n",
      "Epoch 30/90\n",
      "7381/7381 [==============================] - 2s 325us/step - loss: 0.6490 - acc: 0.6904\n",
      "Epoch 31/90\n",
      "7381/7381 [==============================] - 2s 321us/step - loss: 0.6525 - acc: 0.6971\n",
      "Epoch 32/90\n",
      "7381/7381 [==============================] - 2s 323us/step - loss: 0.6487 - acc: 0.6836\n",
      "Epoch 33/90\n",
      "7381/7381 [==============================] - 2s 321us/step - loss: 0.6452 - acc: 0.6967\n",
      "Epoch 34/90\n",
      "7381/7381 [==============================] - 2s 324us/step - loss: 0.6476 - acc: 0.6977\n",
      "Epoch 35/90\n",
      "7381/7381 [==============================] - 2s 326us/step - loss: 0.6265 - acc: 0.7022\n",
      "Epoch 36/90\n",
      "7381/7381 [==============================] - 2s 321us/step - loss: 0.6332 - acc: 0.7053\n",
      "Epoch 37/90\n",
      "7381/7381 [==============================] - 2s 311us/step - loss: 0.6382 - acc: 0.6983\n",
      "Epoch 38/90\n",
      "7381/7381 [==============================] - 2s 323us/step - loss: 0.6350 - acc: 0.6983\n",
      "Epoch 39/90\n",
      "7381/7381 [==============================] - 2s 327us/step - loss: 0.6327 - acc: 0.7052\n",
      "Epoch 40/90\n",
      "7381/7381 [==============================] - 2s 326us/step - loss: 0.6247 - acc: 0.7095\n",
      "Epoch 41/90\n",
      "7381/7381 [==============================] - 2s 332us/step - loss: 0.6279 - acc: 0.7049\n",
      "Epoch 42/90\n",
      "7381/7381 [==============================] - 2s 325us/step - loss: 0.6235 - acc: 0.7084\n",
      "Epoch 43/90\n",
      "7381/7381 [==============================] - 2s 329us/step - loss: 0.6161 - acc: 0.7137\n",
      "Epoch 44/90\n",
      "7381/7381 [==============================] - 2s 318us/step - loss: 0.6108 - acc: 0.7216\n",
      "Epoch 45/90\n",
      "7381/7381 [==============================] - 2s 330us/step - loss: 0.6218 - acc: 0.7072\n",
      "Epoch 46/90\n",
      "7381/7381 [==============================] - 2s 326us/step - loss: 0.6131 - acc: 0.7151\n",
      "Epoch 47/90\n",
      "7381/7381 [==============================] - 2s 325us/step - loss: 0.6042 - acc: 0.7308\n",
      "Epoch 48/90\n",
      "7381/7381 [==============================] - 2s 330us/step - loss: 0.6191 - acc: 0.7093\n",
      "Epoch 49/90\n",
      "7381/7381 [==============================] - 2s 322us/step - loss: 0.6137 - acc: 0.7135\n",
      "Epoch 50/90\n",
      "7381/7381 [==============================] - ETA: 0s - loss: 0.6133 - acc: 0.722 - 2s 326us/step - loss: 0.6141 - acc: 0.7219\n",
      "Epoch 51/90\n",
      "7381/7381 [==============================] - 2s 323us/step - loss: 0.6041 - acc: 0.7232\n",
      "Epoch 52/90\n",
      "7381/7381 [==============================] - 2s 324us/step - loss: 0.5953 - acc: 0.7335\n",
      "Epoch 53/90\n",
      "7381/7381 [==============================] - 2s 315us/step - loss: 0.5972 - acc: 0.7293\n",
      "Epoch 54/90\n",
      "7381/7381 [==============================] - 2s 319us/step - loss: 0.6035 - acc: 0.7200\n",
      "Epoch 55/90\n",
      "7381/7381 [==============================] - 2s 324us/step - loss: 0.5987 - acc: 0.7266\n",
      "Epoch 56/90\n",
      "7381/7381 [==============================] - 2s 320us/step - loss: 0.5968 - acc: 0.7297\n",
      "Epoch 57/90\n",
      "7381/7381 [==============================] - 2s 321us/step - loss: 0.6022 - acc: 0.7286\n",
      "Epoch 58/90\n",
      "7381/7381 [==============================] - 2s 325us/step - loss: 0.6007 - acc: 0.7246\n",
      "Epoch 59/90\n",
      "7381/7381 [==============================] - 2s 322us/step - loss: 0.5892 - acc: 0.7326\n",
      "Epoch 60/90\n",
      "7381/7381 [==============================] - 2s 318us/step - loss: 0.5905 - acc: 0.7301\n",
      "Epoch 61/90\n",
      "7381/7381 [==============================] - 2s 324us/step - loss: 0.6016 - acc: 0.7182\n",
      "Epoch 62/90\n",
      "7381/7381 [==============================] - 2s 322us/step - loss: 0.5875 - acc: 0.7309\n",
      "Epoch 63/90\n",
      "7381/7381 [==============================] - 2s 312us/step - loss: 0.5891 - acc: 0.7385\n",
      "Epoch 64/90\n",
      "7381/7381 [==============================] - 2s 320us/step - loss: 0.5832 - acc: 0.7377\n",
      "Epoch 65/90\n",
      "7381/7381 [==============================] - 2s 326us/step - loss: 0.5899 - acc: 0.7282\n",
      "Epoch 66/90\n",
      "7381/7381 [==============================] - 2s 323us/step - loss: 0.5856 - acc: 0.7316\n",
      "Epoch 67/90\n",
      "7381/7381 [==============================] - 2s 314us/step - loss: 0.5834 - acc: 0.7355\n",
      "Epoch 68/90\n",
      "7381/7381 [==============================] - 2s 326us/step - loss: 0.5836 - acc: 0.7397\n",
      "Epoch 69/90\n",
      "7381/7381 [==============================] - 2s 327us/step - loss: 0.5785 - acc: 0.7403\n",
      "Epoch 70/90\n",
      "7381/7381 [==============================] - 2s 326us/step - loss: 0.5713 - acc: 0.7460\n",
      "Epoch 71/90\n",
      "7381/7381 [==============================] - 2s 318us/step - loss: 0.5741 - acc: 0.7423\n",
      "Epoch 72/90\n",
      "7381/7381 [==============================] - 2s 319us/step - loss: 0.5768 - acc: 0.7437\n",
      "Epoch 73/90\n",
      "7381/7381 [==============================] - 2s 324us/step - loss: 0.5809 - acc: 0.7430\n",
      "Epoch 74/90\n",
      "7381/7381 [==============================] - 2s 323us/step - loss: 0.5767 - acc: 0.7462\n",
      "Epoch 75/90\n",
      "7381/7381 [==============================] - 2s 324us/step - loss: 0.5701 - acc: 0.7460\n",
      "Epoch 76/90\n",
      "7381/7381 [==============================] - 2s 323us/step - loss: 0.5752 - acc: 0.7439\n",
      "Epoch 77/90\n",
      "7381/7381 [==============================] - 2s 319us/step - loss: 0.5738 - acc: 0.7479\n",
      "Epoch 78/90\n",
      "7381/7381 [==============================] - 2s 326us/step - loss: 0.5688 - acc: 0.7464\n",
      "Epoch 79/90\n",
      "7381/7381 [==============================] - 2s 322us/step - loss: 0.5781 - acc: 0.7365\n",
      "Epoch 80/90\n",
      "7381/7381 [==============================] - 2s 323us/step - loss: 0.5624 - acc: 0.7483\n",
      "Epoch 81/90\n",
      "7381/7381 [==============================] - 2s 323us/step - loss: 0.5627 - acc: 0.7534\n",
      "Epoch 82/90\n",
      "7381/7381 [==============================] - 2s 327us/step - loss: 0.5637 - acc: 0.7522\n",
      "Epoch 83/90\n",
      "7381/7381 [==============================] - 2s 323us/step - loss: 0.5645 - acc: 0.7466\n",
      "Epoch 84/90\n",
      "7381/7381 [==============================] - 2s 320us/step - loss: 0.5545 - acc: 0.7502\n",
      "Epoch 85/90\n",
      "7381/7381 [==============================] - 2s 318us/step - loss: 0.5684 - acc: 0.7485\n",
      "Epoch 86/90\n",
      "7381/7381 [==============================] - 2s 326us/step - loss: 0.5590 - acc: 0.7561\n",
      "Epoch 87/90\n",
      "7381/7381 [==============================] - 2s 327us/step - loss: 0.5606 - acc: 0.7479\n",
      "Epoch 88/90\n",
      "7381/7381 [==============================] - 2s 327us/step - loss: 0.5506 - acc: 0.7561\n",
      "Epoch 89/90\n",
      "7381/7381 [==============================] - 2s 319us/step - loss: 0.5557 - acc: 0.7580\n",
      "Epoch 90/90\n",
      "7381/7381 [==============================] - 2s 326us/step - loss: 0.5556 - acc: 0.7556\n",
      "acc: 78.04%\n",
      "77.93% (+/- 1.97%)\n"
     ]
    }
   ],
   "source": [
    "##HAVE TO TEST THE MODEL\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# scores = cross_val_score(stance, input_np, y_np, cv=5, scoring=\"accuracy\")\n",
    "seed = 1\n",
    "kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "\n",
    "cvscores = []\n",
    "for train, test in kfold.split(i3, Y_train1):\n",
    "  # create model\n",
    "    model2 = Sequential()\n",
    "    model2.add(Dense(200, input_shape=(100,),kernel_initializer='he_normal', activation='relu'))\n",
    "    model2.add(Dropout(0.2))\n",
    "    model2.add(Dense(100, kernel_initializer='he_normal', activation='relu'))\n",
    "    model2.add(Dropout(0.2))\n",
    "    model2.add(Dense(50, kernel_initializer='he_normal', activation='relu'))\n",
    "    model2.add(Dropout(0.2))\n",
    "    model2.add(Dense(3, activation='softmax'))\n",
    "    model2.compile(Adam(lr = 0.001),'categorical_crossentropy', metrics=['accuracy'])\n",
    "    # Fit the model\n",
    "#     stance.fit(input_np[train], y_cat[train], epochs=epochs, verbose=1, batch_size=batch_size)\n",
    "    model2.fit(i3[train], y2.values[train],batch_size=16,epochs=90,verbose=1)\n",
    "    # evaluate the model\n",
    "    scores = model2.evaluate(i3[test], y2.values[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model2.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model2_json = model2.to_json()\n",
    "with open(\"contra.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model2.save_weights(\"contra_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_102 (Dense)            (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 45,503\n",
      "Trainable params: 45,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-290-c9f8806b38e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"world\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "a = a.map({\"hello\": 1, \"world\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = open('./sick_vocab.pickle', \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = pickle.load(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"over\" in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataframe(list_heading,list_crawled):\n",
    "    ls = []\n",
    "    c =[]\n",
    "    news_dataset =pd.DataFrame(columns=[\"sentence_A\",\"sentence_B\"])\n",
    "    for heading in range(0,len(list_heading)):\n",
    "        #print(heading,list_heading[heading])\n",
    "        crawled_data = pd.DataFrame()\n",
    "        for crawl in list_crawled[heading]:\n",
    "            ls.append(list_heading[heading])\n",
    "            c.append(crawl)\n",
    "    news_dataset[\"sentence_A\"]=ls\n",
    "    news_dataset[\"sentence_B\"]=c\n",
    "    return news_dataset\n",
    "\n",
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "# Prepare embedding\n",
    "def preprocess(text):\n",
    "    word_list = text_to_word_list(text)\n",
    "    ind = []\n",
    "    for word in word_list : \n",
    "        if(word in vocab) : \n",
    "            ind.append(vocab[word])\n",
    "    return ind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = \"Two children are lying in the snow and are making snow angels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = text_to_word_list(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind = []\n",
    "for word in q : \n",
    "    if(word in vocab) : \n",
    "        ind.append(vocab[word])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 27, 14,\n",
       "        19, 85,  5, 11, 84, 19, 83, 84, 82]], dtype=int32)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences([ind], maxlen=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fro scik"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
